{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New changes to improve baseline accuracy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgZC8AV1o_Ca",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "https://www.kaggle.com/ratan123/start-from-here-disaster-tweets-eda-basic-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMGk9z9KXpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "19017df3-25ae-4ae5-e62e-8273ec88158a"
      },
      "source": [
        "!pip install spellchecker\n",
        "! python -m pip install pyspellchecker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import STOPWORDS\n",
        "from google.colab import drive\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.constraints import maxnorm\n",
        "from keras.callbacks import ModelCheckpoint "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/2f/95ff55a821f6fc83999f8418045ee472edcfd5fb06905090f68fda56a82a/spellchecker-0.4.tar.gz (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spellchecker) (42.0.2)\n",
            "Collecting inexactsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/45/23/0b398af4295da99c5ab69d7b0bff36a2cb68e260a65f64717c17f6a20035/inexactsearch-1.0.2.tar.gz\n",
            "Collecting soundex>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/8f/37b9711595d007e82f70ae6f41b6ab6a1fda406a8321ccfc458fb5023b5f/soundex-1.1.3.tar.gz\n",
            "Collecting silpa_common>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/55/452f5103cb7071d188a818d9e2f12c19c4c8a12124a28aaa212eb6716a4d/silpa_common-0.3.tar.gz\n",
            "Building wheels for collected packages: spellchecker, inexactsearch, soundex, silpa-common\n",
            "  Building wheel for spellchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spellchecker: filename=spellchecker-0.4-cp36-none-any.whl size=3966515 sha256=010f03296a52351a107bd4ca64310ef1f5e05ec34037b1c50e1adfccd8677cf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/e6/ad/28ab959cba7100f5c562a3d1711cd34b630734d241c4e1bd40\n",
            "  Building wheel for inexactsearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inexactsearch: filename=inexactsearch-1.0.2-cp36-none-any.whl size=7113 sha256=502594d219623a2297bc2626352c2dad09e78a97f5072f0af040b97ef28d54da\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/e0/c1/e3fed0e9fd1a3708bc91870fb0ba30ef88527540006763674b\n",
            "  Building wheel for soundex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for soundex: filename=soundex-1.1.3-cp36-none-any.whl size=8863 sha256=980f64f4f816e0fea1350a615457ef255127187c0fb1db9fefd9f63cabc331c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/bb/e6/9a4b6be56c40aa707509bddaf6d414187461ded9db7a25a41a\n",
            "  Building wheel for silpa-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silpa-common: filename=silpa_common-0.3-cp36-none-any.whl size=8453 sha256=5bfba22d768bf5e5dfbc08c7127c3d9e51e4ea5c9d92fca68b82d46916117c29\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/4f/ba/604a82bf904740f1a1d3ad88029c0df5c638bd8825a3cb972d\n",
            "Successfully built spellchecker inexactsearch soundex silpa-common\n",
            "Installing collected packages: silpa-common, soundex, inexactsearch, spellchecker\n",
            "Successfully installed inexactsearch-1.0.2 silpa-common-0.3 soundex-1.1.3 spellchecker-0.4\n",
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/9a570f49dfefc16e9ce1f483bb2d5bff701b95094e051db502e3c11f5092/pyspellchecker-0.5.3-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai2iHXkvKaYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "315853d4-0717-42a3-e6cd-21dac3bc4b63"
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDeva0PfKbyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bb582315-f063-4b70-d28a-bbf5d6be9d2a"
      },
      "source": [
        "cd /content/drive/My Drive/Colab\\ Notebooks"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i83-8jJTKdIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "481bd9ee-2081-4a24-bde7-2d470654cf5d"
      },
      "source": [
        "train_df = pd.read_csv('bert_output/train.csv')\n",
        "test_df = pd.read_csv('bert_output/test.csv')\n",
        "train_df.text.head()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Our Deeds are the Reason of this #earthquake M...\n",
              "1               Forest fire near La Ronge Sask. Canada\n",
              "2    All residents asked to 'shelter in place' are ...\n",
              "3    13,000 people receive #wildfires evacuation or...\n",
              "4    Just got sent this photo from Ruby #Alaska as ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbllbPhKeUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "50951ad5-17ad-4cdb-e86d-a624d4799efe"
      },
      "source": [
        "bert_output = np.load('bert_output/bert-embeddings-train.npy')\n",
        "bert_output_test = np.load('bert_output/bert-embeddings-test.npy')\n",
        "bert_output.shape"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kwCRUGOKfuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "d66ad541-bb7f-4c34-d40d-07274a55c1de"
      },
      "source": [
        "def untokenize(words):\n",
        "    \"\"\"Untokenizing a text undoes the tokenizing operation, restoring\n",
        "    punctuation and spaces to the places that people expect them to be.\n",
        "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
        "    except for line breaks.\n",
        "    \"\"\"\n",
        "    text = ' '.join(words)\n",
        "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
        "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
        "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
        "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
        "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
        "        \"can not\", \"cannot\")\n",
        "    step6 = step5.replace(\" ` \", \" '\")\n",
        "    return step6.strip()\n",
        "\n",
        "def decontracted(phrase):\n",
        "    \"\"\"Convert contractions like \"can't\" into \"can not\"\n",
        "    \"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    #phrase = re.sub(r\"n't\", \" not\", phrase) # resulted in \"ca not\" when sentence started with \"can't\"\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "wl=WordNetLemmatizer()\n",
        "ps=PorterStemmer()\n",
        "sp=stopwords.words('english')\n",
        "def clean_text(tweets):\n",
        "    final_tmp=[]\n",
        "    for tweet in tweets:\n",
        "        #lower and remove punctuation\n",
        "        tweet=tweet.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "        \n",
        "        #Remove Hyperlinks\n",
        "        tweet=re.sub(r'http\\S+','',tweet)\n",
        "        \n",
        "        #Remove numbers and words containing numbers\n",
        "        tweet=' '.join([i for i in tweet.split() if i.isalpha()])\n",
        "        \n",
        "        #Normalize words\n",
        "        tweet=' '.join(wl.lemmatize(i,pos='a') for i in tweet.split())\n",
        "        \n",
        "        #Now stop words\n",
        "        tweet=' '.join(i for i in tweet.split() if i not in sp)\n",
        "        \n",
        "        final_tmp.append(tweet)\n",
        "    return final_tmp\n",
        "\n",
        "\n",
        "# train_df['text'] = clean_text(train_df['text'])\n",
        "# test_df['text'] = clean_text(test_df['text'])\n",
        "train_df.head()"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi55kRUKKjWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slang_abbrev_dict = {\n",
        "    'AFAIK': 'As Far As I Know',\n",
        "    'AFK': 'Away From Keyboard',\n",
        "    'ASAP': 'As Soon As Possible',\n",
        "    'ATK': 'At The Keyboard',\n",
        "    'ATM': 'At The Moment',\n",
        "    'A3': 'Anytime, Anywhere, Anyplace',\n",
        "    'BAK': 'Back At Keyboard',\n",
        "    'BBL': 'Be Back Later',\n",
        "    'BBS': 'Be Back Soon',\n",
        "    'BFN': 'Bye For Now',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'BRB': 'Be Right Back',\n",
        "    'BRT': 'Be Right There',\n",
        "    'BTW': 'By The Way',\n",
        "    'B4': 'Before',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'CU': 'See You',\n",
        "    'CUL8R': 'See You Later',\n",
        "    'CYA': 'See You',\n",
        "    'FAQ': 'Frequently Asked Questions',\n",
        "    'FC': 'Fingers Crossed',\n",
        "    'FWIW': 'For What It\\'s Worth',\n",
        "    'FYI': 'For Your Information',\n",
        "    'GAL': 'Get A Life',\n",
        "    'GG': 'Good Game',\n",
        "    'GN': 'Good Night',\n",
        "    'GMTA': 'Great Minds Think Alike',\n",
        "    'GR8': 'Great!',\n",
        "    'G9': 'Genius',\n",
        "    'IC': 'I See',\n",
        "    'ICQ': 'I Seek you',\n",
        "    'ILU': 'I Love You',\n",
        "    'IMHO': 'In My Humble Opinion',\n",
        "    'IMO': 'In My Opinion',\n",
        "    'IOW': 'In Other Words',\n",
        "    'IRL': 'In Real Life',\n",
        "    'KISS': 'Keep It Simple, Stupid',\n",
        "    'LDR': 'Long Distance Relationship',\n",
        "    'LMAO': 'Laugh My Ass Off',\n",
        "    'LOL': 'Laughing Out Loud',\n",
        "    'LTNS': 'Long Time No See',\n",
        "    'L8R': 'Later',\n",
        "    'MTE': 'My Thoughts Exactly',\n",
        "    'M8': 'Mate',\n",
        "    'NRN': 'No Reply Necessary',\n",
        "    'OIC': 'Oh I See',\n",
        "    'OMG': 'Oh My God',\n",
        "    'PITA': 'Pain In The Ass',\n",
        "    'PRT': 'Party',\n",
        "    'PRW': 'Parents Are Watching',\n",
        "    'QPSA?': 'Que Pasa?',\n",
        "    'ROFL': 'Rolling On The Floor Laughing',\n",
        "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
        "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n",
        "    'SK8': 'Skate',\n",
        "    'STATS': 'Your sex and age',\n",
        "    'ASL': 'Age, Sex, Location',\n",
        "    'THX': 'Thank You',\n",
        "    'TTFN': 'Ta-Ta For Now!',\n",
        "    'TTYL': 'Talk To You Later',\n",
        "    'U': 'You',\n",
        "    'U2': 'You Too',\n",
        "    'U4E': 'Yours For Ever',\n",
        "    'WB': 'Welcome Back',\n",
        "    'WTF': 'What The Fuck',\n",
        "    'WTG': 'Way To Go!',\n",
        "    'WUF': 'Where Are You From?',\n",
        "    'W8': 'Wait',\n",
        "    '7K': 'Sick:-D Laugher'\n",
        "}\n",
        "\n",
        "def unslang(text):\n",
        "    \"\"\"Converts text like \"OMG\" into \"Oh my God\"\n",
        "    \"\"\"\n",
        "    if text.upper() in slang_abbrev_dict.keys():\n",
        "        return slang_abbrev_dict[text.upper()]\n",
        "    else:\n",
        "        return text\n",
        "    \n",
        "def is_slang(text):\n",
        "    if text.upper() in slang_abbrev_dict.keys():\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4qntdHWKl0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def is_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE)\n",
        "    if emoji_pattern.match(text):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uUaS7PfKnTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(reg_exp, text):\n",
        "    text = re.sub(reg_exp, \" \", text)\n",
        "\n",
        "    # replace multiple spaces with one.\n",
        "    text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_urls(text):\n",
        "    text = clean(r\"http\\S+\", text)\n",
        "    text = clean(r\"www\\S+\", text)\n",
        "    text = clean(r\"pic.twitter.com\\S+\", text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjKMFSrfKopN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_all(t):\n",
        "    # first do bulk cleanup on tokens that don't depend on word tokenization\n",
        "    # remove xml tags\n",
        "    t = clean(r\"<[^>]+>\", t)\n",
        "    t = clean(r\"&lt;\", t)\n",
        "    t = clean(r\"&gt;\", t)\n",
        "    # remove URLs\n",
        "    # t = remove_urls(t)\n",
        "    # https://stackoverflow.com/a/35041925\n",
        "    # replace multiple punctuation with single. Ex: !?!?!? would become ?\n",
        "    t = clean(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', t)\n",
        "    # t = remove_emoji(t)\n",
        "    # expand common contractions like \"I'm\" \"he'll\"\n",
        "    t = decontracted(t)\n",
        "    # now remove/expand bad patterns per word\n",
        "    words = word_tokenize(t)\n",
        "    clean_words = []\n",
        "    for w in words:\n",
        "        # normalize punctuation\n",
        "        w = re.sub(r'&', 'and', w)\n",
        "        # expand slang like OMG = Oh my God\n",
        "        w = unslang(w)        \n",
        "        clean_words.append(w)\n",
        "    # join the words back into a full string\n",
        "    t = untokenize(clean_words)\n",
        "    # finally, remove any non ascii and special characters that made it through\n",
        "    t = clean(r\"[^A-Za-z0-9\\.\\'!\\?,\\$]\", t)\n",
        "    return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDPKJdnbKp5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ce57790f-b8fa-4c4a-e332-9ef4b0c9cc31"
      },
      "source": [
        "# train_df['clean_text'] = train_df['text'].apply(lambda x: clean_all(x))\n",
        "# test_df['clean_text'] = test_df['text'].apply(lambda x: clean_all(x))\n",
        "# train_df['text'].iloc[0]"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3HDu2weKsVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spell = SpellChecker()\n",
        "def typos(text):\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    return misspelled_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EqEv3tVKymg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "fea743f5-9107-4c79-caaf-d5b150b01be3"
      },
      "source": [
        "train_df['length_tweet'] =  train_df['text'].apply(lambda x: len(x))\n",
        "test_df['length_tweet'] = test_df['text'].apply(lambda x: len(x))\n",
        "train_df.head()"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>length_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... target length_tweet\n",
              "0   1     NaN  ...      1           69\n",
              "1   4     NaN  ...      1           38\n",
              "2   5     NaN  ...      1          133\n",
              "3   6     NaN  ...      1           65\n",
              "4   7     NaN  ...      1           88\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56sbzYjhSbmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "4f16822d-1956-4bc3-c9f1-7778971b341b"
      },
      "source": [
        "# train_df['length_tweet'] = [len(str(x)) for x in train_df['text'] if x!= 'NaN']\n",
        "\n",
        "pivot = pd.pivot_table(train_df, values='length_tweet', index=['target'], aggfunc=np.mean)\n",
        "pivot"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length_tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>95.706817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>108.113421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        length_tweet\n",
              "target              \n",
              "0          95.706817\n",
              "1         108.113421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GaHIlBjU1Gd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c80377ba-24d8-4b72-9380-691156db1405"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words('english')) \n",
        "\n",
        "stop.add(',')\n",
        "import matplotlib\n",
        "def show_word_distrib(target=1, field=\"text\"):\n",
        "    txt = train_df[train_df['target']==target][field].str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
        "    words = nltk.tokenize.word_tokenize(txt)\n",
        "    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stop) \n",
        "    \n",
        "    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n",
        "                        columns=['Word', 'Frequency']).set_index('Word')\n",
        "    print(rslt)\n",
        "    matplotlib.style.use('ggplot')\n",
        "\n",
        "    rslt.plot.bar(rot=0)\n",
        "    return rslt\n",
        "    \n",
        "print(\"-Number of characters in tweets\")\n",
        "\n",
        "def find_hashtags(tweet):\n",
        "    return \", \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or None\n",
        "\n",
        "def add_hashtags(train_df):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    \n",
        "    train_df['hashtag'] = train_df[\"text\"].apply(lambda x: find_hashtags(x))\n",
        "    train_df['hashtag'].fillna(value=\"no\", inplace=True)\n",
        "\n",
        "    test_df['hashtag'] = test_df[\"text\"].apply(lambda x: find_hashtags(x))\n",
        "    test_df['hashtag'].fillna(value=\"no\", inplace=True)\n",
        "    \n",
        "    return train_df, test_df\n",
        "    \n",
        "top_N = 20\n",
        "\n",
        "train_df,test_df = add_hashtags(train_df)\n",
        "_l = len([v for v in train_df.hashtag.values if isinstance(v, str)])\n",
        "print(\"-Number of tweets with hashtags: {}\".format(_l))\n",
        "print(\"-- Hashtag distribution in positive samples \")\n",
        "show_word_distrib(target=1, field=\"hashtag\")\n",
        "\n",
        "print(\"-- Hashtag distribution in negative samples \")\n",
        "show_word_distrib(target=0, field=\"hashtag\")"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "-Number of characters in tweets\n",
            "-Number of tweets with hashtags: 7613\n",
            "-- Hashtag distribution in positive samples \n",
            "            Frequency\n",
            "Word                 \n",
            "news               56\n",
            "hiroshima          22\n",
            "earthquake         19\n",
            "hot                13\n",
            "prebreak           13\n",
            "best               13\n",
            "japan              11\n",
            "india              10\n",
            "yyc                10\n",
            "breaking            9\n",
            "worldnews           9\n",
            "world               9\n",
            "isis                9\n",
            "sismo               9\n",
            "abstorm             9\n",
            "islam               9\n",
            "disaster            8\n",
            "wildfire            8\n",
            "terrorism           8\n",
            "fukushima           8\n",
            "-- Hashtag distribution in negative samples \n",
            "              Frequency\n",
            "Word                   \n",
            "nowplaying           21\n",
            "news                 20\n",
            "hot                  18\n",
            "prebreak             17\n",
            "best                 17\n",
            "gbbo                 14\n",
            "jobs                 14\n",
            "islam                14\n",
            "job                  12\n",
            "hiring               10\n",
            "fashion               9\n",
            "edm                   8\n",
            "dnb                   8\n",
            "beyhive               8\n",
            "directioners          8\n",
            "emmerdale             8\n",
            "rt                    7\n",
            "dubstep               7\n",
            "trapmusic             7\n",
            "dance                 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Word</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>nowplaying</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>news</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hot</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prebreak</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>best</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gbbo</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jobs</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>islam</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>job</th>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hiring</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>edm</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dnb</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beyhive</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>directioners</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emmerdale</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rt</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dubstep</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trapmusic</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dance</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Frequency\n",
              "Word                   \n",
              "nowplaying           21\n",
              "news                 20\n",
              "hot                  18\n",
              "prebreak             17\n",
              "best                 17\n",
              "gbbo                 14\n",
              "jobs                 14\n",
              "islam                14\n",
              "job                  12\n",
              "hiring               10\n",
              "fashion               9\n",
              "edm                   8\n",
              "dnb                   8\n",
              "beyhive               8\n",
              "directioners          8\n",
              "emmerdale             8\n",
              "rt                    7\n",
              "dubstep               7\n",
              "trapmusic             7\n",
              "dance                 7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYiIySkIRERMTkEgImJy/w9K53x/q+WZ7wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEJCAYAAAByupuRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LniEDSgzWQNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "131333a8-581a-4e90-f58e-885d3ebe494b"
      },
      "source": [
        "top_N = 20\n",
        "target = 1\n",
        "field=\"hashtag\"\n",
        "txt = train_df[train_df['target']==target][field].str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
        "words = nltk.tokenize.word_tokenize(txt)\n",
        "words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stop) \n",
        "\n",
        "rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n",
        "                    columns=['Word', 'Frequency']).set_index('Word')\n",
        "rslt.index"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['news', 'hiroshima', 'earthquake', 'hot', 'prebreak', 'best', 'japan',\n",
              "       'india', 'yyc', 'breaking', 'worldnews', 'world', 'isis', 'sismo',\n",
              "       'abstorm', 'islam', 'disaster', 'wildfire', 'terrorism', 'fukushima'],\n",
              "      dtype='object', name='Word')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL69Cy2aXmaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "07dee652-913d-407e-c34c-d6057b443bd5"
      },
      "source": [
        "hot_words = ['hiroshima', 'earthquake', 'hot', 'prebreak',\n",
        "       'india', 'breaking', 'worldnews', 'world', 'isis', 'sismo',\n",
        "       'abstorm', 'islam', 'disaster', 'wildfire', 'terrorism', 'fukushima','attack','bomb',\n",
        "        'explosion','fire', 'rain', 'tornado' , 'storm', 'murder', 'killing', 'calamity', 'catastrophe', 'collapse',\n",
        "       'crash', 'emergency', 'failure']\n",
        "\n",
        "train_df['word_tokenized'] = train_df['text'].apply(lambda x: word_tokenize(x))\n",
        "test_df['word_tokenized'] = test_df['text'].apply(lambda x: word_tokenize(x))\n",
        "# train_df['hot_word'] = train_df['word_tokenized'].apply(lambda x: [i in hot_words for i in train_df['word_tokenized']] )\n",
        "train_df.head()"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>word_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>no</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>133</td>\n",
              "      <td>no</td>\n",
              "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>wildfires</td>\n",
              "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>Alaska, wildfires</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                     word_tokenized\n",
              "0   1  ...  [Our, Deeds, are, the, Reason, of, this, #, ea...\n",
              "1   4  ...   [Forest, fire, near, La, Ronge, Sask, ., Canada]\n",
              "2   5  ...  [All, residents, asked, to, 'shelter, in, plac...\n",
              "3   6  ...  [13,000, people, receive, #, wildfires, evacua...\n",
              "4   7  ...  [Just, got, sent, this, photo, from, Ruby, #, ...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmweJz1eMbms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "12333dcc-f491-49b5-e3b0-67ac3dc5090e"
      },
      "source": [
        "train_df['is_hot_word'] = train_df['word_tokenized'].apply(lambda x: len(set(x) & set(hot_words)) )\n",
        "test_df['is_hot_word'] = test_df['word_tokenized'].apply(lambda x: len(set(x) & set(hot_words)) )\n",
        "\n",
        "# train_df['is_hot_word'].value_counts()\n",
        "# 0    7165\n",
        "# 1     394\n",
        "# 3      30\n",
        "# 2      24\n",
        "train_df.head(5)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>word_tokenized</th>\n",
              "      <th>is_hot_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>no</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>133</td>\n",
              "      <td>no</td>\n",
              "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>wildfires</td>\n",
              "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>Alaska, wildfires</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                     word_tokenized is_hot_word\n",
              "0   1     NaN  ...  [Our, Deeds, are, the, Reason, of, this, #, ea...           1\n",
              "1   4     NaN  ...   [Forest, fire, near, La, Ronge, Sask, ., Canada]           1\n",
              "2   5     NaN  ...  [All, residents, asked, to, 'shelter, in, plac...           0\n",
              "3   6     NaN  ...  [13,000, people, receive, #, wildfires, evacua...           0\n",
              "4   7     NaN  ...  [Just, got, sent, this, photo, from, Ruby, #, ...           0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 317
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z7MunoTOb2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "3dab5223-c933-40c7-91aa-ee07e6964321"
      },
      "source": [
        "train_df['is_hashtag'] = train_df['hashtag'].apply(lambda x: 1 if x!='no' else 0)\n",
        "test_df['is_hashtag'] = test_df['hashtag'].apply(lambda x: 1 if x!='no' else 0)\n",
        "# train_df['is_hashtag'] .value_counts()\n",
        "# 0    5870\n",
        "# 1    1743\n",
        "pivot = pd.pivot_table(train_df, values='is_hashtag', index=['target'], aggfunc=np.sum)\n",
        "pivot"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_hashtag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>858</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        is_hashtag\n",
              "target            \n",
              "0              885\n",
              "1              858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgbjcj8wXo7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "38d3204a-5521-4df6-f973-94a5d96dc12f"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>word_tokenized</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>happened terrible car crash</td>\n",
              "      <td>27</td>\n",
              "      <td>no</td>\n",
              "      <td>[happened, terrible, car, crash]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>heard earthquake different cities stay safe ev...</td>\n",
              "      <td>52</td>\n",
              "      <td>no</td>\n",
              "      <td>[heard, earthquake, different, cities, stay, s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest fire spot pond geese fleeing across str...</td>\n",
              "      <td>61</td>\n",
              "      <td>no</td>\n",
              "      <td>[forest, fire, spot, pond, geese, fleeing, acr...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apocalypse lighting spokane wildfires</td>\n",
              "      <td>37</td>\n",
              "      <td>no</td>\n",
              "      <td>[apocalypse, lighting, spokane, wildfires]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>typhoon soudelor kills china taiwan</td>\n",
              "      <td>35</td>\n",
              "      <td>no</td>\n",
              "      <td>[typhoon, soudelor, kills, china, taiwan]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... is_hot_word is_hashtag\n",
              "0   0     NaN  ...           1          0\n",
              "1   2     NaN  ...           1          0\n",
              "2   3     NaN  ...           1          0\n",
              "3   9     NaN  ...           0          0\n",
              "4  11     NaN  ...           0          0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7wmqqShU5vj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "90b06116-8daa-4f4d-8300-3edb52bd6a1b"
      },
      "source": [
        "train_df['sentence_count'] = train_df['text'].apply(lambda x: 1 if len(sent_tokenize(x)) > 1 else 0)\n",
        "test_df['sentence_count'] = test_df['text'].apply(lambda x: 1 if len(sent_tokenize(x)) > 1 else 0)\n",
        "\n",
        "\n",
        "pivot = pd.pivot_table(train_df, values='sentence_count', index=['target'], aggfunc=np.sum)\n",
        "pivot"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentence_count\n",
              "target                \n",
              "0                 1826\n",
              "1                  917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCE5h-rXMYRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "c37a4ee9-ba30-4351-c6e9-87b49f863667"
      },
      "source": [
        "#train\n",
        "\n",
        "\n",
        "train_df['location']  = train_df['location'].apply(lambda x: str(x))\n",
        "train_df['keyword']  = train_df['keyword'].apply(lambda x: str(x))\n",
        "\n",
        "train_df['is_location'] = train_df['location'].apply(lambda x: 1 if x!='nan' else 0)\n",
        "train_df['is_location'].value_counts()\n",
        "\n",
        "test_df['location']  = test_df['location'].apply(lambda x: str(x))\n",
        "test_df['keyword']  = test_df['keyword'].apply(lambda x: str(x))\n",
        "\n",
        "test_df['is_location'] = test_df['location'].apply(lambda x: 1 if x!='nan' else 0)\n",
        "test_df['is_location'].value_counts()\n",
        "\n",
        "# pivot = pd.pivot_table(train_df, values='is_location', index=['target'], aggfunc=np.sum)\n",
        "# pivot\n",
        "# is_location\n",
        "# target\t\n",
        "# 0\t2884\n",
        "# 1\t2196\n",
        "\n",
        "train_df['is_keyword'] = train_df['keyword'].apply(lambda x: 1 if x!='nan' else 0)\n",
        "train_df['is_keyword'].value_counts()\n",
        "\n",
        "test_df['is_keyword'] = test_df['keyword'].apply(lambda x: 1 if x!='nan' else 0)\n",
        "test_df['is_keyword'].value_counts()\n",
        "# pivot = pd.pivot_table(train_df, values='is_keyword', index=['target'], aggfunc=np.sum)\n",
        "# pivot\n",
        "# \tis_keyword\n",
        "# target\t\n",
        "# 0\t4323\n",
        "# 1\t3229\n"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3237\n",
              "0      26\n",
              "Name: is_keyword, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qiVJrgzbbNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "cc1c1e56-494a-4127-f24a-e72988965cf9"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>word_tokenized</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>no</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>133</td>\n",
              "      <td>no</td>\n",
              "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>wildfires</td>\n",
              "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>Alaska, wildfires</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location  ... sentence_count  is_location  is_keyword\n",
              "0   1     nan      nan  ...              0            0           0\n",
              "1   4     nan      nan  ...              1            0           0\n",
              "2   5     nan      nan  ...              1            0           0\n",
              "3   6     nan      nan  ...              0            0           0\n",
              "4   7     nan      nan  ...              0            0           0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoCgjUoJlJ0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "6fa71756-f193-4a9f-f88b-84cff8cd3bd6"
      },
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "\n",
        "train = train_df.fillna('No Location')\n",
        "train1_df = train[train[\"target\"]==1]\n",
        "train0_df = train[train[\"target\"]==0]\n",
        "cnt_1 = train1_df['location'].value_counts()\n",
        "cnt_1.reset_index()\n",
        "cnt_1 = cnt_1[:20,]\n",
        "\n",
        "cnt_0 = train0_df['location'].value_counts()\n",
        "cnt_0.reset_index()\n",
        "cnt_0 = cnt_0[:20,]\n",
        "\n",
        "trace1 = go.Bar(\n",
        "                x = cnt_1.index,\n",
        "                y = cnt_1.values,\n",
        "                name = \"Number of tweets referring to disaster\",\n",
        "                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n",
        "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
        "                )\n",
        "trace0 = go.Bar(\n",
        "                x = cnt_0.index,\n",
        "                y = cnt_0.values,\n",
        "                name = \"Number of tweets not referring to disaster\",\n",
        "                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n",
        "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
        "                )\n",
        "\n",
        "\n",
        "data = [trace0,trace1]\n",
        "layout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')\n",
        "fig = go.Figure(data = data, layout = layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ff1bbf5f-d457-431f-be28-491f11153f82\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ff1bbf5f-d457-431f-be28-491f11153f82\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ff1bbf5f-d457-431f-be28-491f11153f82',\n",
              "                        [{\"marker\": {\"color\": \"rgba(79, 82, 97, 0.5)\", \"line\": {\"color\": \"rgb(0,0,0)\", \"width\": 1.5}}, \"name\": \"Number of tweets not referring to disaster\", \"type\": \"bar\", \"x\": [\"nan\", \"New York\", \"USA\", \"London\", \"United States\", \"Los Angeles, CA\", \"Canada\", \"Kenya\", \"Everywhere\", \"UK\", \"Florida\", \"NYC\", \"California\", \"United Kingdom\", \"Australia\", \"Chicago, IL\", \"London, England\", \"ss\", \"304\", \"San Francisco\"], \"y\": [1458, 55, 37, 29, 23, 18, 16, 15, 12, 11, 11, 10, 10, 10, 9, 9, 9, 9, 9, 8]}, {\"marker\": {\"color\": \"rgba(255, 74, 55, 0.5)\", \"line\": {\"color\": \"rgb(0,0,0)\", \"width\": 1.5}}, \"name\": \"Number of tweets referring to disaster\", \"type\": \"bar\", \"x\": [\"nan\", \"USA\", \"United States\", \"Nigeria\", \"India\", \"Mumbai\", \"London\", \"New York\", \"UK\", \"Washington, DC\", \"Canada\", \"Worldwide\", \"Chicago, IL\", \"Washington, D.C.\", \"Australia\", \"Indonesia\", \"California, USA\", \"Los Angeles, CA\", \"Earth\", \"New York, NY\"], \"y\": [1075, 67, 27, 22, 20, 19, 16, 16, 16, 15, 13, 12, 9, 9, 9, 8, 8, 8, 8, 7]}],\n",
              "                        {\"barmode\": \"stack\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Number of tweets in dataset according to location\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ff1bbf5f-d457-431f-be28-491f11153f82');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTAm-DIzmDUZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "TWEET LOCATION\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0bPHkX482oX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "8901f8ef-620a-40c8-f946-bd6beb5ed270"
      },
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "df = train['location'].value_counts()[:20,]\n",
        "df = pd.DataFrame(df)\n",
        "df = df.reset_index()\n",
        "df.columns = ['location', 'counts'] \n",
        "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
        "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
        "dictt_latitude = {}\n",
        "dictt_longitude = {}\n",
        "for i in df['location'].values:\n",
        "    print(i)\n",
        "    location = geocode(i)\n",
        "    dictt_latitude[i] = location.latitude\n",
        "    dictt_longitude[i] = location.longitude\n",
        "df['latitude']= df['location'].map(dictt_latitude)\n",
        "df['longitude'] = df['location'].map(dictt_longitude)"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            "USA\n",
            "New York\n",
            "United States\n",
            "London\n",
            "Canada\n",
            "Nigeria\n",
            "UK\n",
            "Los Angeles, CA\n",
            "India\n",
            "Mumbai\n",
            "Washington, DC\n",
            "Kenya\n",
            "Worldwide\n",
            "Australia\n",
            "Chicago, IL\n",
            "California\n",
            "Everywhere\n",
            "New York, NY\n",
            "California, USA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9VfO2EWLzox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "028e3406-d971-41e5-9874-18e1cfe6e90e"
      },
      "source": [
        "import folium \n",
        "from folium import plugins \n",
        "map1 = folium.Map(location=[10.0, 10.0], tiles='CartoDB dark_matter', zoom_start=2.3)\n",
        "markers = []\n",
        "for i, row in df.iterrows():\n",
        "    loss = row['counts']\n",
        "    if row['counts'] > 0:\n",
        "        count = row['counts']*0.4\n",
        "    folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='#ef4f61', fill=True).add_to(map1)\n",
        "map1"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_498fd58d015b451ab8437dedbe9f2d83 {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
</head>
<body>    
    
    <div class="folium-map" id="map_498fd58d015b451ab8437dedbe9f2d83" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_498fd58d015b451ab8437dedbe9f2d83 = L.map(
        'map_498fd58d015b451ab8437dedbe9f2d83', {
        center: [10.0, 10.0],
        zoom: 2.3,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_6eb53dd6bfbb45d1b5781c9ed9840f32 = L.tileLayer(
        'https://cartodb-basemaps-{s}.global.ssl.fastly.net/dark_all/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_498fd58d015b451ab8437dedbe9f2d83);
    
            var circle_marker_42431f9dbfc94a3e82e0e799fa82d4ef = L.circleMarker(
                [34.220389, 70.3800314],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 1013.2,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_6fbc846fbaae407aa85b03e03da743b3 = L.circleMarker(
                [39.7837304, -100.4458825],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 41.6,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_fa8a2106e4d94c118f03435a12f7050e = L.circleMarker(
                [40.7127281, -74.0060152],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 28.400000000000002,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_ab536d6375374aa2a99497c5c5324e4a = L.circleMarker(
                [39.7837304, -100.4458825],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 20.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_023c00851aa9401aa2109fb22a33c9ce = L.circleMarker(
                [51.5073219, -0.1276474],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 18.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_8c52b8a3b0894387a307df1eb9bacba6 = L.circleMarker(
                [61.0666922, -107.9917071],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 11.600000000000001,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_13ee292850454e9bb15429ee10e346e5 = L.circleMarker(
                [9.6000359, 7.9999721],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 11.200000000000001,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_9c40f5eeabca457abe5075b820b3a705 = L.circleMarker(
                [54.7023545, -3.2765753],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 10.8,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_08cca02650c040b9b9f14e819c4937e0 = L.circleMarker(
                [34.0536909, -118.2427666],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 10.4,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_772b978075b34047854440ffa2a940db = L.circleMarker(
                [22.3511148, 78.6677428],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 9.600000000000001,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_25d6932702f24b558e7a71bc6ab871ae = L.circleMarker(
                [18.9387711, 72.8353355],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 8.8,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_39303c7c0f804c7ea34acaecf7751023 = L.circleMarker(
                [38.8948932, -77.0365529],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 8.4,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_6e4182c710724a08b707cd930cea71c4 = L.circleMarker(
                [1.4419683, 38.4313975],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 8.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_fdb275bbbe904c4dbbab49e95d7f8e21 = L.circleMarker(
                [53.5120412, -2.2434612],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 7.6000000000000005,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_c4aee6f7989645559137f9025df33b55 = L.circleMarker(
                [-24.7761086, 134.755],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 7.2,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_b65b6e43206b455dab579212674d3925 = L.circleMarker(
                [41.8755616, -87.6244212],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 7.2,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_88fd765c5f934f5695752bc471839552 = L.circleMarker(
                [36.7014631, -118.7559974],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 6.800000000000001,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_4b03c0c4a4e7467b941a5130a4068334 = L.circleMarker(
                [46.9599823, 22.0541733],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 6.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_6b3fa63da54049859da2fe358d47288e = L.circleMarker(
                [40.7127281, -74.0060152],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 6.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
    
            var circle_marker_a85284a34d404b11a76dd7191d2cc5df = L.circleMarker(
                [36.7014631, -118.7559974],
                {
  "bubblingMouseEvents": true,
  "color": "#ef4f61",
  "dashArray": null,
  "dashOffset": null,
  "fill": true,
  "fillColor": "#ef4f61",
  "fillOpacity": 0.2,
  "fillRule": "evenodd",
  "lineCap": "round",
  "lineJoin": "round",
  "opacity": 1.0,
  "radius": 6.0,
  "stroke": true,
  "weight": 3
}
                )
                .addTo(map_498fd58d015b451ab8437dedbe9f2d83);
            
</script>\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f74ee782c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg5hwSiNMAXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "90d441ca-45b1-435d-d00d-a0adf1d7ac84"
      },
      "source": [
        "# delete text, clean text, keyword, location\n",
        "train_df_model = train_df.drop(['id','location','keyword', 'hashtag'], axis = 1)\n",
        "train_df_model['long_tweet'] = train_df_model['length_tweet'].apply(lambda x: 1 if x>28 else 0)\n",
        "# train_df_model.drop('length_tweet',axis = 1, inplace = True)\n",
        "# test_df_model = test_df.drop(['id','location','keyword','text','clean_text'], axis = 1)\n",
        "train_df_model.head()\n",
        "\n",
        "\n",
        "test_df_model = test_df.drop(['location','keyword', 'hashtag'], axis = 1)\n",
        "test_df_model['long_tweet'] = test_df_model['length_tweet'].apply(lambda x: 1 if x>28 else 0)\n",
        "# train_df_model.drop('length_tweet',axis = 1, inplace = True)\n",
        "# test_df_model = test_df.drop(['id','location','keyword','text','clean_text'], axis = 1)\n",
        "test_df_model.head()\n",
        "\n",
        "# pivot = pd.pivot_table(train_df_model, values='long_tweet', index=['target'], aggfunc=np.sum)\n",
        "# pivot\n",
        "\n",
        "# \tlong_tweet\n",
        "# target\t\n",
        "# 0\t4086\n",
        "# 1\t3208"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>word_tokenized</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "      <th>long_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>34</td>\n",
              "      <td>[Just, happened, a, terrible, car, crash]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>64</td>\n",
              "      <td>[Heard, about, #, earthquake, is, different, c...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>96</td>\n",
              "      <td>[there, is, a, forest, fire, at, spot, pond, ,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>40</td>\n",
              "      <td>[Apocalypse, lighting, ., #, Spokane, #, wildf...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>45</td>\n",
              "      <td>[Typhoon, Soudelor, kills, 28, in, China, and,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... long_tweet\n",
              "0   0  ...          1\n",
              "1   2  ...          1\n",
              "2   3  ...          1\n",
              "3   9  ...          1\n",
              "4  11  ...          1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFm716xOnn1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e5b234c-8e3c-4795-8b60-6fbb1aa23685"
      },
      "source": [
        "for sent in train0_df[\"text\"]:\n",
        "    for word in generate_ngrams(sent,2):\n",
        "        freq_dict[word] += 1\n",
        "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
        "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
        "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
        "\n",
        "\n",
        "freq_dict = defaultdict(int)\n",
        "for sent in train1_df[\"text\"]:\n",
        "    for word in generate_ngrams(sent,2):\n",
        "        freq_dict[word] += 1\n",
        "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
        "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
        "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
        "\n",
        "# Creating two subplots\n",
        "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
        "                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n",
        "                                          \"Frequent words if tweet is real disaster\"])\n",
        "fig.append_trace(trace0, 1, 1)\n",
        "fig.append_trace(trace1, 1, 2)\n",
        "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\n",
        "py.iplot(fig, filename='word-plots')"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/plotly/tools.py:465: DeprecationWarning:\n",
            "\n",
            "plotly.tools.make_subplots is deprecated, please use plotly.subplots.make_subplots instead\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"2405174e-09e1-4b95-93b6-cdded8d435bd\" class=\"plotly-graph-div\" style=\"height:1200px; width:900px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"2405174e-09e1-4b95-93b6-cdded8d435bd\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '2405174e-09e1-4b95-93b6-cdded8d435bd',\n",
              "                        [{\"marker\": {\"color\": \"green\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"x\": [23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 28, 28, 28, 28, 29, 29, 30, 31, 34, 34, 34, 34, 39, 39, 41, 56, 59, 60], \"xaxis\": \"x\", \"y\": [\"content policy\", \"stock market\", \"body bag\", \"body bagging\", \"first responders\", \"watch airport\", \"airport swallowed\", \"swallowed sandstorm\", \"sandstorm minute\", \"confirmed malaysia\", \"full read\", \"obama declares\", \"declares disaster\", \"disaster typhoondevastated\", \"typhoondevastated saipan\", \"dust storm\", \"legionnaires families\", \"wreckage conclusively\", \"conclusively confirmed\", \"malaysia pm\", \"full re\\u0089\\u00fb_\", \"debris found\", \"families sue\", \"sue legionnaires\", \"families affected\", \"affected fatal\", \"fatal outbreak\", \"outbreak legionnaires\", \"wild fires\", \"heat wave\", \"severe thunderstorm\", \"old pkk\", \"late homes\", \"razed northern\", \"pkk suicide\", \"detonated bomb\", \"natural disaster\", \"homes razed\", \"bomber detonated\", \"mass murder\", \"suicide bombing\", \"california wildfire\", \"liked @youtube\", \"@youtube video\", \"cross body\", \"oil spill\", \"northern california\", \"burning buildings\", \"suicide bomber\", \"- full\"], \"yaxis\": \"y\"}, {\"marker\": {\"color\": \"green\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"x\": [21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 27, 27, 28, 28, 28, 28, 28, 29, 30, 30, 30, 31, 37, 40, 59], \"xaxis\": \"x2\", \"y\": [\"mass murder\", \"army trench\", \"wounded suspect\", \"typhoon-devastated saipan:\", \"hundreds migrants\", \"charged manslaughter\", \"refugio oil\", \"spill may\", \"may costlier\", \"costlier bigger\", \"bomb turkey\", \"turkey army\", \"officer wounded\", \"outbreak legionnaires'\", \"watch airport\", \"airport swallowed\", \"swallowed sandstorm\", \"sandstorm minute\", \"confirmed' mh370:\", \"mh370: malaysia\", \"burning buildings\", \"obama declares\", \"declares disaster\", \"disaster typhoon-devastated\", \"severe thunderstorm\", \"sue legionnaires:\", \"legionnaires: 40\", \"wreckage 'conclusively\", \"'conclusively confirmed'\", \"debris found\", \"families sue\", \"40 families\", \"families affected\", \"affected fatal\", \"fatal outbreak\", \"wildfire -\", \"old pkk\", \"latest: homes\", \"razed northern\", \"16yr old\", \"pkk suicide\", \"detonated bomb\", \"homes razed\", \"70 years\", \"suicide bombing\", \"bomber detonated\", \"california wildfire\", \"oil spill\", \"northern california\", \"suicide bomber\"], \"yaxis\": \"y2\"}],\n",
              "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent words if tweet is not real disaster\", \"x\": 0.2125, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent words if tweet is real disaster\", \"x\": 0.7875, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"height\": 1200, \"paper_bgcolor\": \"rgb(233,233,233)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Bigram Count Plots\"}, \"width\": 900, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.425]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.575, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0]}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2405174e-09e1-4b95-93b6-cdded8d435bd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFDP5uQwpS3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_hot_bigrams = ['suicide bomber',\n",
        " 'northern california',\n",
        " 'oil spill',\n",
        " 'california wildfire',\n",
        " 'bomber detonated',\n",
        " 'suicide bombing',\n",
        " 'homes razed',\n",
        " 'detonated bomb',\n",
        " 'pkk suicide',\n",
        " 'razed northern',\n",
        " 'wildfire',\n",
        " 'fatal outbreak',\n",
        " 'affected fatal',\n",
        " 'families affected',\n",
        " 'families sue',\n",
        " 'debris found',\n",
        " \"wreckage 'conclusively\",\n",
        " 'sue legionnaires:',\n",
        " 'severe thunderstorm',\n",
        " 'disaster typhoon-devastated',\n",
        " 'declares disaster',\n",
        " 'obama declares',\n",
        " 'burning buildings',\n",
        " 'mh370: malaysia',\n",
        " \"confirmed' mh370:\",\n",
        " 'sandstorm minute',\n",
        " 'swallowed sandstorm',\n",
        " 'airport swallowed',\n",
        " 'watch airport',\n",
        " \"outbreak legionnaires'\",\n",
        " 'officer wounded',\n",
        " 'turkey army',\n",
        " 'bomb turkey',\n",
        " 'costlier bigger',\n",
        " 'may costlier',\n",
        " 'spill may',\n",
        " 'refugio oil',\n",
        " 'charged manslaughter',\n",
        " 'hundreds migrants',\n",
        " 'typhoon-devastated saipan:',\n",
        " 'wounded suspect',\n",
        " 'army trench',\n",
        " 'mass murder',\n",
        " 'thunderstorm warning',\n",
        " 'investigators families',\n",
        " 'pm: investigators',\n",
        " 'malaysia pm:',\n",
        " 'trench released',\n",
        " 'police officer',\n",
        " 'bigger projected',\n",
        " 'boy charged',\n",
        " 'signs disaster',\n",
        " 'obama signs',\n",
        " 'saipan: obama',\n",
        " 'rescuers searching',\n",
        " 'heat wave',\n",
        " 'searching hundreds',\n",
        " \"legionnaires' disea...\",\n",
        " 'disaster declaration',\n",
        " 'wild fires',\n",
        " 'bomber kills',\n",
        " 'structural failure',\n",
        " 'picking bodies',\n",
        " 'bridge collapse',\n",
        " 'pic 16yr',\n",
        " 'virgin galactic',\n",
        " \"water': rescuers\",\n",
        " \"bodies water':\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9B_1-Bkje05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "bf1fe0f5-52ce-49fa-a379-62c26ac7d280"
      },
      "source": [
        "# text = [\"this is a sentence\"]#, \"so is this one\"]\n",
        "# bigrams = [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "# list_of_bigrams = [list(i) for i in bigrams]\n",
        "# [\" \".join(i) for i in list_of_bigrams]\n",
        "\n",
        "bigrams = [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "list_of_bigrams = [list(i) for i in bigrams]\n",
        "[\" \".join(i) for i in list_of_bigrams]\n",
        "\n",
        "train_df_model['bigrams'] = train_df_model['text'].apply(lambda x:  [\" \".join(list(b)) for l in [x] for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])])\n",
        "train_df_model.head()\n",
        "\n",
        "test_df_model['bigrams'] = test_df_model['text'].apply(lambda x:  [\" \".join(list(b)) for l in [x] for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])])\n",
        "test_df_model.head()"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>length_tweet</th>\n",
              "      <th>word_tokenized</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "      <th>long_tweet</th>\n",
              "      <th>bigrams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>34</td>\n",
              "      <td>[Just, happened, a, terrible, car, crash]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Just happened, happened a, a terrible, terrib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>64</td>\n",
              "      <td>[Heard, about, #, earthquake, is, different, c...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Heard about, about #earthquake, #earthquake i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>96</td>\n",
              "      <td>[there, is, a, forest, fire, at, spot, pond, ,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[there is, is a, a forest, forest fire, fire a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>40</td>\n",
              "      <td>[Apocalypse, lighting, ., #, Spokane, #, wildf...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Apocalypse lighting., lighting. #Spokane, #Sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>45</td>\n",
              "      <td>[Typhoon, Soudelor, kills, 28, in, China, and,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Typhoon Soudelor, Soudelor kills, kills 28, 2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                            bigrams\n",
              "0   0  ...  [Just happened, happened a, a terrible, terrib...\n",
              "1   2  ...  [Heard about, about #earthquake, #earthquake i...\n",
              "2   3  ...  [there is, is a, a forest, forest fire, fire a...\n",
              "3   9  ...  [Apocalypse lighting., lighting. #Spokane, #Sp...\n",
              "4  11  ...  [Typhoon Soudelor, Soudelor kills, kills 28, 2...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZvfN-bCSFQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "2361a9f9-b647-49fd-c714-992359a3d04e"
      },
      "source": [
        "train_df_model['is_hot_bigram'] = train_df_model['bigrams'].apply(lambda x: len(set(x) & set(list_of_hot_bigrams)) )\n",
        "train_df_model['is_hot_bigram'] = train_df_model['is_hot_bigram'].apply(lambda x: 1 if x>0 else 0)\n",
        "# train_df['is_hot_bigram'].value_counts()\n",
        "train_df_model.drop(['text','bigrams','word_tokenized','length_tweet'],axis = 1, inplace = True)\n",
        "train_df_model.head()\n",
        "\n",
        "\n",
        "test_df_model['is_hot_bigram'] = test_df_model['bigrams'].apply(lambda x: len(set(x) & set(list_of_hot_bigrams)) )\n",
        "test_df_model['is_hot_bigram'] = test_df_model['is_hot_bigram'].apply(lambda x: 1 if x>0 else 0)\n",
        "# train_df['is_hot_bigram'].value_counts()\n",
        "test_df_model.drop(['text','bigrams','word_tokenized','length_tweet'],axis = 1, inplace = True)\n",
        "test_df_model.head()\n",
        "\n",
        "# pivot = pd.pivot_table(train_df_model, values='is_hot_bigram', index=['target'], aggfunc=np.sum)\n",
        "# pivot\n",
        "\n",
        "# \tis_hot_bigram\n",
        "# target\t\n",
        "# 0\t32\n",
        "# 1\t282"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "      <th>long_tweet</th>\n",
              "      <th>is_hot_bigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  is_hot_word  is_hashtag  ...  is_keyword  long_tweet  is_hot_bigram\n",
              "0   0            1           0  ...           0           1              0\n",
              "1   2            1           1  ...           0           1              0\n",
              "2   3            1           0  ...           0           1              0\n",
              "3   9            0           1  ...           0           1              0\n",
              "4  11            0           0  ...           0           1              0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gWdrxenMJo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert bert output into a dataframe for both train and test\n",
        "bert_output = pd.DataFrame(bert_output)\n",
        "bert_output_test = pd.DataFrame(bert_output_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6ZpRBCLMRrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "69eafb05-1b57-4333-9219-e89d3aa1d56a"
      },
      "source": [
        "data_for_training = pd.concat([train_df_model.iloc[:,1:], bert_output], axis=1, sort=False)\n",
        "data_for_testing = pd.concat([test_df_model.iloc[:,0:], bert_output_test], axis=1, sort=False)\n",
        "data_for_testing.head()\n",
        "\n",
        "# data_for_training = train_df_model.iloc[:,1:]\n",
        "# data_for_training.head()"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "      <th>long_tweet</th>\n",
              "      <th>is_hot_bigram</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.206292</td>\n",
              "      <td>-0.113055</td>\n",
              "      <td>0.257145</td>\n",
              "      <td>-0.037640</td>\n",
              "      <td>0.173348</td>\n",
              "      <td>-0.406878</td>\n",
              "      <td>0.207133</td>\n",
              "      <td>0.610017</td>\n",
              "      <td>0.132386</td>\n",
              "      <td>-0.301216</td>\n",
              "      <td>0.145209</td>\n",
              "      <td>0.016320</td>\n",
              "      <td>-0.128272</td>\n",
              "      <td>0.347116</td>\n",
              "      <td>-0.124173</td>\n",
              "      <td>0.162992</td>\n",
              "      <td>0.263492</td>\n",
              "      <td>-0.184704</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.216337</td>\n",
              "      <td>0.290866</td>\n",
              "      <td>0.021114</td>\n",
              "      <td>-0.148166</td>\n",
              "      <td>-0.102380</td>\n",
              "      <td>0.149735</td>\n",
              "      <td>0.364254</td>\n",
              "      <td>-0.286867</td>\n",
              "      <td>0.190481</td>\n",
              "      <td>-0.310768</td>\n",
              "      <td>0.150234</td>\n",
              "      <td>0.095056</td>\n",
              "      <td>0.018479</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.372508</td>\n",
              "      <td>-0.101734</td>\n",
              "      <td>0.145190</td>\n",
              "      <td>0.223967</td>\n",
              "      <td>-0.215068</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>-0.379860</td>\n",
              "      <td>0.139731</td>\n",
              "      <td>-0.046657</td>\n",
              "      <td>0.343576</td>\n",
              "      <td>-0.188617</td>\n",
              "      <td>0.308684</td>\n",
              "      <td>-0.109651</td>\n",
              "      <td>-0.016755</td>\n",
              "      <td>-0.291082</td>\n",
              "      <td>0.003755</td>\n",
              "      <td>0.249891</td>\n",
              "      <td>-0.092941</td>\n",
              "      <td>0.041237</td>\n",
              "      <td>-0.093183</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>-0.159965</td>\n",
              "      <td>-0.274375</td>\n",
              "      <td>-0.603380</td>\n",
              "      <td>0.204265</td>\n",
              "      <td>-0.236663</td>\n",
              "      <td>-0.239486</td>\n",
              "      <td>0.293508</td>\n",
              "      <td>-0.029808</td>\n",
              "      <td>-0.340842</td>\n",
              "      <td>0.428433</td>\n",
              "      <td>-0.125875</td>\n",
              "      <td>-0.023025</td>\n",
              "      <td>-0.133087</td>\n",
              "      <td>0.077934</td>\n",
              "      <td>-0.135723</td>\n",
              "      <td>0.090791</td>\n",
              "      <td>-0.197440</td>\n",
              "      <td>0.058177</td>\n",
              "      <td>-0.145312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.126378</td>\n",
              "      <td>-0.151372</td>\n",
              "      <td>0.481115</td>\n",
              "      <td>-0.038587</td>\n",
              "      <td>0.184135</td>\n",
              "      <td>-0.462109</td>\n",
              "      <td>0.594098</td>\n",
              "      <td>0.509235</td>\n",
              "      <td>-0.246784</td>\n",
              "      <td>-0.177184</td>\n",
              "      <td>-0.078313</td>\n",
              "      <td>-0.208663</td>\n",
              "      <td>-0.163508</td>\n",
              "      <td>0.342248</td>\n",
              "      <td>-0.410779</td>\n",
              "      <td>-0.102764</td>\n",
              "      <td>0.201341</td>\n",
              "      <td>0.063469</td>\n",
              "      <td>-0.039832</td>\n",
              "      <td>0.445843</td>\n",
              "      <td>0.325865</td>\n",
              "      <td>0.037662</td>\n",
              "      <td>0.343607</td>\n",
              "      <td>-0.200406</td>\n",
              "      <td>0.083355</td>\n",
              "      <td>-0.329768</td>\n",
              "      <td>-0.173463</td>\n",
              "      <td>0.232066</td>\n",
              "      <td>-0.229411</td>\n",
              "      <td>-0.036296</td>\n",
              "      <td>-0.061546</td>\n",
              "      <td>0.432358</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.223250</td>\n",
              "      <td>-0.169165</td>\n",
              "      <td>-0.101317</td>\n",
              "      <td>0.088923</td>\n",
              "      <td>-0.173761</td>\n",
              "      <td>-0.212447</td>\n",
              "      <td>-0.438114</td>\n",
              "      <td>0.185388</td>\n",
              "      <td>-0.316497</td>\n",
              "      <td>-0.038279</td>\n",
              "      <td>-0.226286</td>\n",
              "      <td>-0.171124</td>\n",
              "      <td>0.169253</td>\n",
              "      <td>-0.122906</td>\n",
              "      <td>-0.227463</td>\n",
              "      <td>0.125426</td>\n",
              "      <td>-0.145592</td>\n",
              "      <td>-0.092487</td>\n",
              "      <td>0.020526</td>\n",
              "      <td>0.005939</td>\n",
              "      <td>0.453325</td>\n",
              "      <td>-0.029890</td>\n",
              "      <td>-0.493776</td>\n",
              "      <td>0.158884</td>\n",
              "      <td>0.404596</td>\n",
              "      <td>0.169981</td>\n",
              "      <td>0.008827</td>\n",
              "      <td>0.235254</td>\n",
              "      <td>-0.129269</td>\n",
              "      <td>-0.295105</td>\n",
              "      <td>-0.020475</td>\n",
              "      <td>-0.165873</td>\n",
              "      <td>0.219995</td>\n",
              "      <td>0.231785</td>\n",
              "      <td>0.158701</td>\n",
              "      <td>-0.056250</td>\n",
              "      <td>0.168198</td>\n",
              "      <td>-0.303525</td>\n",
              "      <td>-0.013686</td>\n",
              "      <td>-0.199653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.208732</td>\n",
              "      <td>0.076510</td>\n",
              "      <td>0.320086</td>\n",
              "      <td>0.170445</td>\n",
              "      <td>0.125228</td>\n",
              "      <td>-0.278019</td>\n",
              "      <td>0.207956</td>\n",
              "      <td>0.401073</td>\n",
              "      <td>-0.182761</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.149643</td>\n",
              "      <td>-0.080398</td>\n",
              "      <td>-0.236592</td>\n",
              "      <td>0.491937</td>\n",
              "      <td>-0.174556</td>\n",
              "      <td>0.110451</td>\n",
              "      <td>-0.091751</td>\n",
              "      <td>0.246112</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.216716</td>\n",
              "      <td>0.076013</td>\n",
              "      <td>-0.160012</td>\n",
              "      <td>0.058508</td>\n",
              "      <td>0.218086</td>\n",
              "      <td>0.321029</td>\n",
              "      <td>-0.102378</td>\n",
              "      <td>-0.232576</td>\n",
              "      <td>0.113893</td>\n",
              "      <td>-0.173693</td>\n",
              "      <td>-0.112878</td>\n",
              "      <td>0.091271</td>\n",
              "      <td>0.192828</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.107810</td>\n",
              "      <td>-0.294061</td>\n",
              "      <td>-0.018130</td>\n",
              "      <td>0.225852</td>\n",
              "      <td>-0.399272</td>\n",
              "      <td>-0.033576</td>\n",
              "      <td>-0.159763</td>\n",
              "      <td>0.050007</td>\n",
              "      <td>0.002339</td>\n",
              "      <td>0.222264</td>\n",
              "      <td>-0.089890</td>\n",
              "      <td>0.166502</td>\n",
              "      <td>0.045417</td>\n",
              "      <td>-0.019429</td>\n",
              "      <td>-0.104035</td>\n",
              "      <td>0.175564</td>\n",
              "      <td>0.188577</td>\n",
              "      <td>-0.344213</td>\n",
              "      <td>0.021531</td>\n",
              "      <td>-0.358125</td>\n",
              "      <td>0.275160</td>\n",
              "      <td>-0.150722</td>\n",
              "      <td>-0.206600</td>\n",
              "      <td>0.099167</td>\n",
              "      <td>0.120465</td>\n",
              "      <td>-0.245994</td>\n",
              "      <td>-0.005484</td>\n",
              "      <td>-0.073102</td>\n",
              "      <td>-0.172824</td>\n",
              "      <td>-0.333555</td>\n",
              "      <td>-0.053223</td>\n",
              "      <td>-0.027466</td>\n",
              "      <td>-0.107598</td>\n",
              "      <td>-0.077475</td>\n",
              "      <td>-0.139521</td>\n",
              "      <td>0.049713</td>\n",
              "      <td>-0.136232</td>\n",
              "      <td>0.077064</td>\n",
              "      <td>0.075558</td>\n",
              "      <td>0.115107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.123669</td>\n",
              "      <td>-0.257363</td>\n",
              "      <td>0.033196</td>\n",
              "      <td>0.008619</td>\n",
              "      <td>-0.017399</td>\n",
              "      <td>-0.103638</td>\n",
              "      <td>0.456081</td>\n",
              "      <td>0.326836</td>\n",
              "      <td>-0.230241</td>\n",
              "      <td>-0.466926</td>\n",
              "      <td>-0.080469</td>\n",
              "      <td>-0.032098</td>\n",
              "      <td>-0.495967</td>\n",
              "      <td>0.242116</td>\n",
              "      <td>-0.190685</td>\n",
              "      <td>-0.164652</td>\n",
              "      <td>-0.030599</td>\n",
              "      <td>0.202306</td>\n",
              "      <td>0.141944</td>\n",
              "      <td>0.355540</td>\n",
              "      <td>0.097313</td>\n",
              "      <td>-0.227458</td>\n",
              "      <td>0.048716</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-0.201166</td>\n",
              "      <td>0.260034</td>\n",
              "      <td>-0.096229</td>\n",
              "      <td>0.021641</td>\n",
              "      <td>-0.462344</td>\n",
              "      <td>0.425936</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.264927</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.273580</td>\n",
              "      <td>-0.482840</td>\n",
              "      <td>0.258288</td>\n",
              "      <td>0.282566</td>\n",
              "      <td>-0.515495</td>\n",
              "      <td>-0.202641</td>\n",
              "      <td>-0.263403</td>\n",
              "      <td>0.029889</td>\n",
              "      <td>0.088701</td>\n",
              "      <td>0.473376</td>\n",
              "      <td>-0.011192</td>\n",
              "      <td>0.120483</td>\n",
              "      <td>-0.443588</td>\n",
              "      <td>-0.052380</td>\n",
              "      <td>-0.415964</td>\n",
              "      <td>-0.261790</td>\n",
              "      <td>-0.139862</td>\n",
              "      <td>0.245748</td>\n",
              "      <td>0.261483</td>\n",
              "      <td>-0.030431</td>\n",
              "      <td>0.052081</td>\n",
              "      <td>-0.306504</td>\n",
              "      <td>-0.273728</td>\n",
              "      <td>-0.020792</td>\n",
              "      <td>0.097888</td>\n",
              "      <td>0.174486</td>\n",
              "      <td>-0.323708</td>\n",
              "      <td>0.046917</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>-0.057291</td>\n",
              "      <td>0.344007</td>\n",
              "      <td>0.094371</td>\n",
              "      <td>-0.231564</td>\n",
              "      <td>-0.021173</td>\n",
              "      <td>0.259172</td>\n",
              "      <td>-0.309689</td>\n",
              "      <td>0.283516</td>\n",
              "      <td>0.162922</td>\n",
              "      <td>0.034134</td>\n",
              "      <td>-0.109642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.244448</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.056073</td>\n",
              "      <td>0.177915</td>\n",
              "      <td>-0.116873</td>\n",
              "      <td>0.027654</td>\n",
              "      <td>0.035792</td>\n",
              "      <td>0.454270</td>\n",
              "      <td>-0.086762</td>\n",
              "      <td>0.078437</td>\n",
              "      <td>-0.186529</td>\n",
              "      <td>-0.461408</td>\n",
              "      <td>-0.255728</td>\n",
              "      <td>0.120699</td>\n",
              "      <td>-0.171494</td>\n",
              "      <td>0.199815</td>\n",
              "      <td>-0.145918</td>\n",
              "      <td>0.098352</td>\n",
              "      <td>-0.009424</td>\n",
              "      <td>-0.153551</td>\n",
              "      <td>0.241185</td>\n",
              "      <td>-0.291973</td>\n",
              "      <td>0.094620</td>\n",
              "      <td>0.202216</td>\n",
              "      <td>0.073311</td>\n",
              "      <td>-0.192961</td>\n",
              "      <td>-0.155130</td>\n",
              "      <td>0.180534</td>\n",
              "      <td>-0.288508</td>\n",
              "      <td>-0.206734</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>0.045502</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.398336</td>\n",
              "      <td>-0.143743</td>\n",
              "      <td>0.106398</td>\n",
              "      <td>0.293327</td>\n",
              "      <td>-0.298150</td>\n",
              "      <td>-0.030243</td>\n",
              "      <td>-0.068640</td>\n",
              "      <td>-0.139920</td>\n",
              "      <td>0.025015</td>\n",
              "      <td>0.497330</td>\n",
              "      <td>-0.341690</td>\n",
              "      <td>0.156943</td>\n",
              "      <td>-0.084615</td>\n",
              "      <td>0.226769</td>\n",
              "      <td>0.209428</td>\n",
              "      <td>-0.470335</td>\n",
              "      <td>0.177939</td>\n",
              "      <td>0.054949</td>\n",
              "      <td>-0.359366</td>\n",
              "      <td>-0.063119</td>\n",
              "      <td>-0.164627</td>\n",
              "      <td>-0.354907</td>\n",
              "      <td>-0.324136</td>\n",
              "      <td>-0.053469</td>\n",
              "      <td>-0.013127</td>\n",
              "      <td>-0.406428</td>\n",
              "      <td>-0.287756</td>\n",
              "      <td>0.113375</td>\n",
              "      <td>-0.484681</td>\n",
              "      <td>-0.263360</td>\n",
              "      <td>0.346316</td>\n",
              "      <td>0.216065</td>\n",
              "      <td>0.105335</td>\n",
              "      <td>0.148456</td>\n",
              "      <td>-0.006926</td>\n",
              "      <td>0.009776</td>\n",
              "      <td>-0.055775</td>\n",
              "      <td>-0.018340</td>\n",
              "      <td>0.206965</td>\n",
              "      <td>-0.192643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 776 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  is_hot_word  is_hashtag  ...       765       766       767\n",
              "0   0            1           0  ... -0.197440  0.058177 -0.145312\n",
              "1   2            1           1  ... -0.303525 -0.013686 -0.199653\n",
              "2   3            1           0  ...  0.077064  0.075558  0.115107\n",
              "3   9            0           1  ...  0.162922  0.034134 -0.109642\n",
              "4  11            0           0  ... -0.018340  0.206965 -0.192643\n",
              "\n",
              "[5 rows x 776 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGzOnupDMTk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_for_training  #(7613, 785)\n",
        "y = train_df_model.iloc[:,0] # (7613,)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIudK6lMh26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_mlp(X_train, y_train, X_test, y_test, epochs = 200, batch_size = 256 , verbose = 1):\n",
        "    ''' define mlp architecture '''\n",
        "    mlp = Sequential()\n",
        "    mlp.add(Dense(60, input_dim = X_train.shape[1], kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    mlp.add(Dropout(0.25))\n",
        "\n",
        "    mlp.add(Dense(30,  activation='relu'))\n",
        "    mlp.add(Dropout(0.25))\n",
        "    \n",
        "    mlp.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    mlp.summary() # display the summary of the mlp architecture\n",
        "    \n",
        "    checkpointer = ModelCheckpoint(filepath = 'mlp-weights-best.hdf5',  monitor = 'val_acc',\n",
        "                                   verbose = verbose, save_best_only=True)\n",
        "    \n",
        "    mlp.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
        "            epochs = epochs, batch_size = batch_size, callbacks = [checkpointer], \n",
        "            verbose = verbose)\n",
        "    \n",
        "    return mlp\n",
        "\n",
        "def mlp_predict(X_test, mlp):\n",
        "        ''' mlp predict '''\n",
        "        return mlp.predict_proba(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiLll0LRMnjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93300967-b8e4-4d13-ff86-6863a704ce0d"
      },
      "source": [
        "mlp = train_mlp(X_train, y_train, X_test, y_test, epochs = 200, batch_size = 256, verbose = 1)"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_45 (Dense)             (None, 60)                46560     \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 60)                0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 48,421\n",
            "Trainable params: 48,421\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 5329 samples, validate on 2284 samples\n",
            "Epoch 1/200\n",
            "5329/5329 [==============================] - 2s 361us/step - loss: 0.5639 - acc: 0.7257 - val_loss: 0.4410 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.80867, saving model to mlp-weights-best.hdf5\n",
            "Epoch 2/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.4695 - acc: 0.7891 - val_loss: 0.4115 - val_acc: 0.8249\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.80867 to 0.82487, saving model to mlp-weights-best.hdf5\n",
            "Epoch 3/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.4342 - acc: 0.8122 - val_loss: 0.4015 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.82487 to 0.82662, saving model to mlp-weights-best.hdf5\n",
            "Epoch 4/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.4263 - acc: 0.8161 - val_loss: 0.4045 - val_acc: 0.8236\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.82662\n",
            "Epoch 5/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.4097 - acc: 0.8238 - val_loss: 0.3976 - val_acc: 0.8271\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.82662 to 0.82706, saving model to mlp-weights-best.hdf5\n",
            "Epoch 6/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.4015 - acc: 0.8309 - val_loss: 0.3965 - val_acc: 0.8314\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.82706 to 0.83144, saving model to mlp-weights-best.hdf5\n",
            "Epoch 7/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.3904 - acc: 0.8381 - val_loss: 0.4009 - val_acc: 0.8279\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.83144\n",
            "Epoch 8/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.3839 - acc: 0.8373 - val_loss: 0.4082 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.83144\n",
            "Epoch 9/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.3773 - acc: 0.8411 - val_loss: 0.3956 - val_acc: 0.8297\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.83144\n",
            "Epoch 10/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.3726 - acc: 0.8465 - val_loss: 0.3956 - val_acc: 0.8310\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.83144\n",
            "Epoch 11/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.3724 - acc: 0.8450 - val_loss: 0.3953 - val_acc: 0.8310\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.83144\n",
            "Epoch 12/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.3651 - acc: 0.8503 - val_loss: 0.3935 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.83144 to 0.83669, saving model to mlp-weights-best.hdf5\n",
            "Epoch 13/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.3451 - acc: 0.8540 - val_loss: 0.4006 - val_acc: 0.8363\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.83669\n",
            "Epoch 14/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.3366 - acc: 0.8596 - val_loss: 0.4104 - val_acc: 0.8297\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.83669\n",
            "Epoch 15/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.3343 - acc: 0.8608 - val_loss: 0.4057 - val_acc: 0.8292\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.83669\n",
            "Epoch 16/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.3228 - acc: 0.8636 - val_loss: 0.4245 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.83669\n",
            "Epoch 17/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.3200 - acc: 0.8660 - val_loss: 0.4160 - val_acc: 0.8341\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.83669\n",
            "Epoch 18/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.3078 - acc: 0.8730 - val_loss: 0.4239 - val_acc: 0.8292\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.83669\n",
            "Epoch 19/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.2943 - acc: 0.8801 - val_loss: 0.4259 - val_acc: 0.8292\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.83669\n",
            "Epoch 20/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.2846 - acc: 0.8833 - val_loss: 0.4239 - val_acc: 0.8306\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.83669\n",
            "Epoch 21/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.2790 - acc: 0.8906 - val_loss: 0.4386 - val_acc: 0.8284\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.83669\n",
            "Epoch 22/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.2694 - acc: 0.8932 - val_loss: 0.4367 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.83669\n",
            "Epoch 23/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.2529 - acc: 0.9007 - val_loss: 0.4404 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.83669\n",
            "Epoch 24/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.2427 - acc: 0.9050 - val_loss: 0.4608 - val_acc: 0.8227\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.83669\n",
            "Epoch 25/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.2402 - acc: 0.9101 - val_loss: 0.4656 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.83669\n",
            "Epoch 26/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.2385 - acc: 0.9017 - val_loss: 0.4806 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.83669\n",
            "Epoch 27/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.2164 - acc: 0.9172 - val_loss: 0.4834 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.83669\n",
            "Epoch 28/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.2048 - acc: 0.9214 - val_loss: 0.5079 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.83669\n",
            "Epoch 29/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.1989 - acc: 0.9229 - val_loss: 0.5223 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.83669\n",
            "Epoch 30/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.1863 - acc: 0.9309 - val_loss: 0.5186 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.83669\n",
            "Epoch 31/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.1837 - acc: 0.9293 - val_loss: 0.5193 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.83669\n",
            "Epoch 32/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.1728 - acc: 0.9366 - val_loss: 0.5645 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.83669\n",
            "Epoch 33/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.1637 - acc: 0.9428 - val_loss: 0.5566 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.83669\n",
            "Epoch 34/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.1629 - acc: 0.9392 - val_loss: 0.5773 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.83669\n",
            "Epoch 35/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.1565 - acc: 0.9430 - val_loss: 0.5505 - val_acc: 0.8179\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.83669\n",
            "Epoch 36/200\n",
            "5329/5329 [==============================] - 0s 43us/step - loss: 0.1458 - acc: 0.9476 - val_loss: 0.6016 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.83669\n",
            "Epoch 37/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.1381 - acc: 0.9521 - val_loss: 0.6007 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.83669\n",
            "Epoch 38/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.1353 - acc: 0.9533 - val_loss: 0.5947 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.83669\n",
            "Epoch 39/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.1397 - acc: 0.9467 - val_loss: 0.6185 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.83669\n",
            "Epoch 40/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.1324 - acc: 0.9495 - val_loss: 0.6230 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.83669\n",
            "Epoch 41/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.1212 - acc: 0.9578 - val_loss: 0.6170 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.83669\n",
            "Epoch 42/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.1146 - acc: 0.9612 - val_loss: 0.6324 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.83669\n",
            "Epoch 43/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.1169 - acc: 0.9589 - val_loss: 0.6712 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.83669\n",
            "Epoch 44/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.1097 - acc: 0.9595 - val_loss: 0.6753 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.83669\n",
            "Epoch 45/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.1037 - acc: 0.9645 - val_loss: 0.7030 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.83669\n",
            "Epoch 46/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.1016 - acc: 0.9632 - val_loss: 0.7144 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.83669\n",
            "Epoch 47/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0980 - acc: 0.9640 - val_loss: 0.7222 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.83669\n",
            "Epoch 48/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0975 - acc: 0.9657 - val_loss: 0.6948 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.83669\n",
            "Epoch 49/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0952 - acc: 0.9673 - val_loss: 0.6870 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.83669\n",
            "Epoch 50/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0908 - acc: 0.9670 - val_loss: 0.7590 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.83669\n",
            "Epoch 51/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0921 - acc: 0.9670 - val_loss: 0.7367 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.83669\n",
            "Epoch 52/200\n",
            "5329/5329 [==============================] - 0s 35us/step - loss: 0.0944 - acc: 0.9670 - val_loss: 0.7343 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.83669\n",
            "Epoch 53/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.0889 - acc: 0.9687 - val_loss: 0.7732 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.83669\n",
            "Epoch 54/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0808 - acc: 0.9707 - val_loss: 0.7638 - val_acc: 0.8034\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.83669\n",
            "Epoch 55/200\n",
            "5329/5329 [==============================] - 0s 35us/step - loss: 0.0824 - acc: 0.9668 - val_loss: 0.7433 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.83669\n",
            "Epoch 56/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0823 - acc: 0.9707 - val_loss: 0.7809 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.83669\n",
            "Epoch 57/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0749 - acc: 0.9752 - val_loss: 0.7834 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.83669\n",
            "Epoch 58/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0835 - acc: 0.9717 - val_loss: 0.7634 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.83669\n",
            "Epoch 59/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0821 - acc: 0.9724 - val_loss: 0.7883 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.83669\n",
            "Epoch 60/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0787 - acc: 0.9722 - val_loss: 0.8172 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.83669\n",
            "Epoch 61/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0818 - acc: 0.9685 - val_loss: 0.8048 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.83669\n",
            "Epoch 62/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0775 - acc: 0.9704 - val_loss: 0.8065 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.83669\n",
            "Epoch 63/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0724 - acc: 0.9743 - val_loss: 0.8735 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.83669\n",
            "Epoch 64/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0738 - acc: 0.9728 - val_loss: 0.7737 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.83669\n",
            "Epoch 65/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0720 - acc: 0.9743 - val_loss: 0.8393 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.83669\n",
            "Epoch 66/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0681 - acc: 0.9732 - val_loss: 0.8489 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.83669\n",
            "Epoch 67/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0710 - acc: 0.9741 - val_loss: 0.8253 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.83669\n",
            "Epoch 68/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0713 - acc: 0.9737 - val_loss: 0.8272 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.83669\n",
            "Epoch 69/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0740 - acc: 0.9719 - val_loss: 0.8393 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.83669\n",
            "Epoch 70/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0674 - acc: 0.9767 - val_loss: 0.8288 - val_acc: 0.7986\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.83669\n",
            "Epoch 71/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.0623 - acc: 0.9775 - val_loss: 0.9192 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.83669\n",
            "Epoch 72/200\n",
            "5329/5329 [==============================] - 0s 43us/step - loss: 0.0623 - acc: 0.9756 - val_loss: 0.8735 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.83669\n",
            "Epoch 73/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0605 - acc: 0.9764 - val_loss: 0.8830 - val_acc: 0.8148\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.83669\n",
            "Epoch 74/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0619 - acc: 0.9775 - val_loss: 0.8668 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.83669\n",
            "Epoch 75/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0618 - acc: 0.9779 - val_loss: 0.9116 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.83669\n",
            "Epoch 76/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0606 - acc: 0.9786 - val_loss: 0.8899 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.83669\n",
            "Epoch 77/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0592 - acc: 0.9777 - val_loss: 0.9300 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.83669\n",
            "Epoch 78/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0617 - acc: 0.9775 - val_loss: 0.8997 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.83669\n",
            "Epoch 79/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0609 - acc: 0.9773 - val_loss: 0.9283 - val_acc: 0.8187\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.83669\n",
            "Epoch 80/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0629 - acc: 0.9773 - val_loss: 0.8644 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.83669\n",
            "Epoch 81/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0608 - acc: 0.9779 - val_loss: 0.9381 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.83669\n",
            "Epoch 82/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0613 - acc: 0.9771 - val_loss: 0.9165 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.83669\n",
            "Epoch 83/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0647 - acc: 0.9767 - val_loss: 0.9707 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.83669\n",
            "Epoch 84/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0572 - acc: 0.9795 - val_loss: 0.9275 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.83669\n",
            "Epoch 85/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0598 - acc: 0.9764 - val_loss: 0.9925 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.83669\n",
            "Epoch 86/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0648 - acc: 0.9756 - val_loss: 0.9564 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.83669\n",
            "Epoch 87/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0613 - acc: 0.9764 - val_loss: 0.9366 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.83669\n",
            "Epoch 88/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0560 - acc: 0.9786 - val_loss: 0.9342 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.83669\n",
            "Epoch 89/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0574 - acc: 0.9784 - val_loss: 0.9686 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.83669\n",
            "Epoch 90/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0585 - acc: 0.9775 - val_loss: 0.9484 - val_acc: 0.7977\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.83669\n",
            "Epoch 91/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0572 - acc: 0.9803 - val_loss: 0.9560 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.83669\n",
            "Epoch 92/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0557 - acc: 0.9794 - val_loss: 0.9615 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.83669\n",
            "Epoch 93/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0535 - acc: 0.9786 - val_loss: 1.0241 - val_acc: 0.7938\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.83669\n",
            "Epoch 94/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0560 - acc: 0.9799 - val_loss: 0.9481 - val_acc: 0.8201\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.83669\n",
            "Epoch 95/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0551 - acc: 0.9779 - val_loss: 0.9962 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.83669\n",
            "Epoch 96/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0538 - acc: 0.9797 - val_loss: 0.9805 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.83669\n",
            "Epoch 97/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0533 - acc: 0.9799 - val_loss: 1.0121 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.83669\n",
            "Epoch 98/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0533 - acc: 0.9790 - val_loss: 0.9989 - val_acc: 0.8039\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.83669\n",
            "Epoch 99/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0474 - acc: 0.9812 - val_loss: 0.9959 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.83669\n",
            "Epoch 100/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0464 - acc: 0.9816 - val_loss: 1.0540 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.83669\n",
            "Epoch 101/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0566 - acc: 0.9792 - val_loss: 0.9692 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.83669\n",
            "Epoch 102/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0505 - acc: 0.9794 - val_loss: 1.0417 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.83669\n",
            "Epoch 103/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0465 - acc: 0.9829 - val_loss: 1.0489 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.83669\n",
            "Epoch 104/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.0452 - acc: 0.9825 - val_loss: 1.0349 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.83669\n",
            "Epoch 105/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0470 - acc: 0.9799 - val_loss: 1.0058 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.83669\n",
            "Epoch 106/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0511 - acc: 0.9805 - val_loss: 1.0179 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.83669\n",
            "Epoch 107/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.0472 - acc: 0.9810 - val_loss: 1.0863 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.83669\n",
            "Epoch 108/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0461 - acc: 0.9799 - val_loss: 1.0557 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.83669\n",
            "Epoch 109/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0500 - acc: 0.9803 - val_loss: 1.0410 - val_acc: 0.8165\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.83669\n",
            "Epoch 110/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0487 - acc: 0.9799 - val_loss: 1.0454 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.83669\n",
            "Epoch 111/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0465 - acc: 0.9814 - val_loss: 1.0531 - val_acc: 0.8039\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.83669\n",
            "Epoch 112/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0449 - acc: 0.9807 - val_loss: 1.0455 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.83669\n",
            "Epoch 113/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0463 - acc: 0.9820 - val_loss: 1.0649 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.83669\n",
            "Epoch 114/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0467 - acc: 0.9816 - val_loss: 1.0653 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.83669\n",
            "Epoch 115/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0403 - acc: 0.9829 - val_loss: 1.1390 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.83669\n",
            "Epoch 116/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0480 - acc: 0.9824 - val_loss: 1.0601 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.83669\n",
            "Epoch 117/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0469 - acc: 0.9805 - val_loss: 1.0514 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.83669\n",
            "Epoch 118/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0469 - acc: 0.9790 - val_loss: 1.0672 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.83669\n",
            "Epoch 119/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0510 - acc: 0.9788 - val_loss: 1.0788 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.83669\n",
            "Epoch 120/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0478 - acc: 0.9812 - val_loss: 1.1075 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.83669\n",
            "Epoch 121/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0439 - acc: 0.9803 - val_loss: 1.0569 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.83669\n",
            "Epoch 122/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0465 - acc: 0.9805 - val_loss: 1.0596 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.83669\n",
            "Epoch 123/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0442 - acc: 0.9831 - val_loss: 1.0825 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.83669\n",
            "Epoch 124/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0429 - acc: 0.9820 - val_loss: 1.1444 - val_acc: 0.8039\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.83669\n",
            "Epoch 125/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0410 - acc: 0.9820 - val_loss: 1.0725 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.83669\n",
            "Epoch 126/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0394 - acc: 0.9839 - val_loss: 1.0895 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.83669\n",
            "Epoch 127/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0396 - acc: 0.9812 - val_loss: 1.1327 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.83669\n",
            "Epoch 128/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0462 - acc: 0.9801 - val_loss: 1.0915 - val_acc: 0.7903\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.83669\n",
            "Epoch 129/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0427 - acc: 0.9824 - val_loss: 1.0750 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.83669\n",
            "Epoch 130/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0451 - acc: 0.9816 - val_loss: 1.1721 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.83669\n",
            "Epoch 131/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0492 - acc: 0.9777 - val_loss: 1.0482 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.83669\n",
            "Epoch 132/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0448 - acc: 0.9786 - val_loss: 1.1417 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.83669\n",
            "Epoch 133/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0442 - acc: 0.9810 - val_loss: 1.0647 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.83669\n",
            "Epoch 134/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0501 - acc: 0.9794 - val_loss: 1.0855 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.83669\n",
            "Epoch 135/200\n",
            "5329/5329 [==============================] - 0s 43us/step - loss: 0.0414 - acc: 0.9829 - val_loss: 1.1052 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.83669\n",
            "Epoch 136/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0503 - acc: 0.9797 - val_loss: 1.1178 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.83669\n",
            "Epoch 137/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0443 - acc: 0.9816 - val_loss: 1.1392 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.83669\n",
            "Epoch 138/200\n",
            "5329/5329 [==============================] - 0s 42us/step - loss: 0.0415 - acc: 0.9840 - val_loss: 1.1454 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.83669\n",
            "Epoch 139/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0428 - acc: 0.9831 - val_loss: 1.1235 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.83669\n",
            "Epoch 140/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0350 - acc: 0.9840 - val_loss: 1.1642 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.83669\n",
            "Epoch 141/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0387 - acc: 0.9844 - val_loss: 1.1524 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.83669\n",
            "Epoch 142/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0409 - acc: 0.9820 - val_loss: 1.1232 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.83669\n",
            "Epoch 143/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0370 - acc: 0.9839 - val_loss: 1.1872 - val_acc: 0.7982\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.83669\n",
            "Epoch 144/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0402 - acc: 0.9835 - val_loss: 1.1249 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.83669\n",
            "Epoch 145/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0407 - acc: 0.9822 - val_loss: 1.1983 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.83669\n",
            "Epoch 146/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0416 - acc: 0.9831 - val_loss: 1.1884 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.83669\n",
            "Epoch 147/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0407 - acc: 0.9822 - val_loss: 1.1728 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.83669\n",
            "Epoch 148/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0410 - acc: 0.9822 - val_loss: 1.1544 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.83669\n",
            "Epoch 149/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0393 - acc: 0.9844 - val_loss: 1.2189 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.83669\n",
            "Epoch 150/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0395 - acc: 0.9835 - val_loss: 1.2131 - val_acc: 0.7955\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.83669\n",
            "Epoch 151/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0415 - acc: 0.9803 - val_loss: 1.2309 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.83669\n",
            "Epoch 152/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0433 - acc: 0.9816 - val_loss: 1.1740 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.83669\n",
            "Epoch 153/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0396 - acc: 0.9846 - val_loss: 1.1584 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.83669\n",
            "Epoch 154/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0402 - acc: 0.9822 - val_loss: 1.1727 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.83669\n",
            "Epoch 155/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0387 - acc: 0.9829 - val_loss: 1.2655 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.83669\n",
            "Epoch 156/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0381 - acc: 0.9827 - val_loss: 1.1544 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.83669\n",
            "Epoch 157/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0397 - acc: 0.9824 - val_loss: 1.1740 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.83669\n",
            "Epoch 158/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0410 - acc: 0.9814 - val_loss: 1.1835 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.83669\n",
            "Epoch 159/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0384 - acc: 0.9833 - val_loss: 1.1933 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.83669\n",
            "Epoch 160/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0395 - acc: 0.9829 - val_loss: 1.2021 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.83669\n",
            "Epoch 161/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0378 - acc: 0.9829 - val_loss: 1.2556 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.83669\n",
            "Epoch 162/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0420 - acc: 0.9814 - val_loss: 1.2203 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.83669\n",
            "Epoch 163/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0437 - acc: 0.9807 - val_loss: 1.2120 - val_acc: 0.8157\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.83669\n",
            "Epoch 164/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0417 - acc: 0.9814 - val_loss: 1.1351 - val_acc: 0.8039\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.83669\n",
            "Epoch 165/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0409 - acc: 0.9818 - val_loss: 1.2665 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.83669\n",
            "Epoch 166/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0394 - acc: 0.9835 - val_loss: 1.2489 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.83669\n",
            "Epoch 167/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0380 - acc: 0.9848 - val_loss: 1.2217 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.83669\n",
            "Epoch 168/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0441 - acc: 0.9812 - val_loss: 1.1648 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.83669\n",
            "Epoch 169/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0362 - acc: 0.9825 - val_loss: 1.2507 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.83669\n",
            "Epoch 170/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0368 - acc: 0.9842 - val_loss: 1.1804 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.83669\n",
            "Epoch 171/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0366 - acc: 0.9835 - val_loss: 1.2610 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.83669\n",
            "Epoch 172/200\n",
            "5329/5329 [==============================] - 0s 35us/step - loss: 0.0357 - acc: 0.9842 - val_loss: 1.2629 - val_acc: 0.8157\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.83669\n",
            "Epoch 173/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0361 - acc: 0.9852 - val_loss: 1.2309 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.83669\n",
            "Epoch 174/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0396 - acc: 0.9827 - val_loss: 1.2846 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.83669\n",
            "Epoch 175/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0372 - acc: 0.9842 - val_loss: 1.2997 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.83669\n",
            "Epoch 176/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0375 - acc: 0.9844 - val_loss: 1.2602 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.83669\n",
            "Epoch 177/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0380 - acc: 0.9824 - val_loss: 1.2250 - val_acc: 0.8161\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.83669\n",
            "Epoch 178/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0442 - acc: 0.9810 - val_loss: 1.2422 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.83669\n",
            "Epoch 179/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0426 - acc: 0.9809 - val_loss: 1.2119 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.83669\n",
            "Epoch 180/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0417 - acc: 0.9822 - val_loss: 1.2720 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.83669\n",
            "Epoch 181/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0390 - acc: 0.9839 - val_loss: 1.2256 - val_acc: 0.8165\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.83669\n",
            "Epoch 182/200\n",
            "5329/5329 [==============================] - 0s 43us/step - loss: 0.0375 - acc: 0.9837 - val_loss: 1.2654 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.83669\n",
            "Epoch 183/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0350 - acc: 0.9848 - val_loss: 1.2757 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.83669\n",
            "Epoch 184/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0400 - acc: 0.9837 - val_loss: 1.2643 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.83669\n",
            "Epoch 185/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0428 - acc: 0.9797 - val_loss: 1.2380 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.83669\n",
            "Epoch 186/200\n",
            "5329/5329 [==============================] - 0s 35us/step - loss: 0.0409 - acc: 0.9816 - val_loss: 1.2532 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.83669\n",
            "Epoch 187/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0397 - acc: 0.9829 - val_loss: 1.2940 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.83669\n",
            "Epoch 188/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0342 - acc: 0.9833 - val_loss: 1.2805 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.83669\n",
            "Epoch 189/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0362 - acc: 0.9824 - val_loss: 1.3034 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.83669\n",
            "Epoch 190/200\n",
            "5329/5329 [==============================] - 0s 41us/step - loss: 0.0388 - acc: 0.9820 - val_loss: 1.3157 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.83669\n",
            "Epoch 191/200\n",
            "5329/5329 [==============================] - 0s 36us/step - loss: 0.0393 - acc: 0.9827 - val_loss: 1.2954 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.83669\n",
            "Epoch 192/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0481 - acc: 0.9799 - val_loss: 1.2255 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.83669\n",
            "Epoch 193/200\n",
            "5329/5329 [==============================] - 0s 38us/step - loss: 0.0430 - acc: 0.9822 - val_loss: 1.2323 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.83669\n",
            "Epoch 194/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0377 - acc: 0.9844 - val_loss: 1.2933 - val_acc: 0.7938\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.83669\n",
            "Epoch 195/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0350 - acc: 0.9831 - val_loss: 1.2574 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.83669\n",
            "Epoch 196/200\n",
            "5329/5329 [==============================] - 0s 37us/step - loss: 0.0375 - acc: 0.9839 - val_loss: 1.2833 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.83669\n",
            "Epoch 197/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0418 - acc: 0.9809 - val_loss: 1.2401 - val_acc: 0.8126\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.83669\n",
            "Epoch 198/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0423 - acc: 0.9820 - val_loss: 1.3507 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.83669\n",
            "Epoch 199/200\n",
            "5329/5329 [==============================] - 0s 39us/step - loss: 0.0411 - acc: 0.9805 - val_loss: 1.1743 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.83669\n",
            "Epoch 200/200\n",
            "5329/5329 [==============================] - 0s 40us/step - loss: 0.0399 - acc: 0.9818 - val_loss: 1.2687 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.83669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkueI5MiMoNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4e9ed81c-6acb-467b-f176-649fe6018142"
      },
      "source": [
        "mlp.load_weights('mlp-weights-best.hdf5') # 0.83669\n",
        "data_for_testing.head()"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>is_hot_word</th>\n",
              "      <th>is_hashtag</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>is_location</th>\n",
              "      <th>is_keyword</th>\n",
              "      <th>long_tweet</th>\n",
              "      <th>is_hot_bigram</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.206292</td>\n",
              "      <td>-0.113055</td>\n",
              "      <td>0.257145</td>\n",
              "      <td>-0.037640</td>\n",
              "      <td>0.173348</td>\n",
              "      <td>-0.406878</td>\n",
              "      <td>0.207133</td>\n",
              "      <td>0.610017</td>\n",
              "      <td>0.132386</td>\n",
              "      <td>-0.301216</td>\n",
              "      <td>0.145209</td>\n",
              "      <td>0.016320</td>\n",
              "      <td>-0.128272</td>\n",
              "      <td>0.347116</td>\n",
              "      <td>-0.124173</td>\n",
              "      <td>0.162992</td>\n",
              "      <td>0.263492</td>\n",
              "      <td>-0.184704</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.216337</td>\n",
              "      <td>0.290866</td>\n",
              "      <td>0.021114</td>\n",
              "      <td>-0.148166</td>\n",
              "      <td>-0.102380</td>\n",
              "      <td>0.149735</td>\n",
              "      <td>0.364254</td>\n",
              "      <td>-0.286867</td>\n",
              "      <td>0.190481</td>\n",
              "      <td>-0.310768</td>\n",
              "      <td>0.150234</td>\n",
              "      <td>0.095056</td>\n",
              "      <td>0.018479</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.372508</td>\n",
              "      <td>-0.101734</td>\n",
              "      <td>0.145190</td>\n",
              "      <td>0.223967</td>\n",
              "      <td>-0.215068</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>-0.379860</td>\n",
              "      <td>0.139731</td>\n",
              "      <td>-0.046657</td>\n",
              "      <td>0.343576</td>\n",
              "      <td>-0.188617</td>\n",
              "      <td>0.308684</td>\n",
              "      <td>-0.109651</td>\n",
              "      <td>-0.016755</td>\n",
              "      <td>-0.291082</td>\n",
              "      <td>0.003755</td>\n",
              "      <td>0.249891</td>\n",
              "      <td>-0.092941</td>\n",
              "      <td>0.041237</td>\n",
              "      <td>-0.093183</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>-0.159965</td>\n",
              "      <td>-0.274375</td>\n",
              "      <td>-0.603380</td>\n",
              "      <td>0.204265</td>\n",
              "      <td>-0.236663</td>\n",
              "      <td>-0.239486</td>\n",
              "      <td>0.293508</td>\n",
              "      <td>-0.029808</td>\n",
              "      <td>-0.340842</td>\n",
              "      <td>0.428433</td>\n",
              "      <td>-0.125875</td>\n",
              "      <td>-0.023025</td>\n",
              "      <td>-0.133087</td>\n",
              "      <td>0.077934</td>\n",
              "      <td>-0.135723</td>\n",
              "      <td>0.090791</td>\n",
              "      <td>-0.197440</td>\n",
              "      <td>0.058177</td>\n",
              "      <td>-0.145312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.126378</td>\n",
              "      <td>-0.151372</td>\n",
              "      <td>0.481115</td>\n",
              "      <td>-0.038587</td>\n",
              "      <td>0.184135</td>\n",
              "      <td>-0.462109</td>\n",
              "      <td>0.594098</td>\n",
              "      <td>0.509235</td>\n",
              "      <td>-0.246784</td>\n",
              "      <td>-0.177184</td>\n",
              "      <td>-0.078313</td>\n",
              "      <td>-0.208663</td>\n",
              "      <td>-0.163508</td>\n",
              "      <td>0.342248</td>\n",
              "      <td>-0.410779</td>\n",
              "      <td>-0.102764</td>\n",
              "      <td>0.201341</td>\n",
              "      <td>0.063469</td>\n",
              "      <td>-0.039832</td>\n",
              "      <td>0.445843</td>\n",
              "      <td>0.325865</td>\n",
              "      <td>0.037662</td>\n",
              "      <td>0.343607</td>\n",
              "      <td>-0.200406</td>\n",
              "      <td>0.083355</td>\n",
              "      <td>-0.329768</td>\n",
              "      <td>-0.173463</td>\n",
              "      <td>0.232066</td>\n",
              "      <td>-0.229411</td>\n",
              "      <td>-0.036296</td>\n",
              "      <td>-0.061546</td>\n",
              "      <td>0.432358</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.223250</td>\n",
              "      <td>-0.169165</td>\n",
              "      <td>-0.101317</td>\n",
              "      <td>0.088923</td>\n",
              "      <td>-0.173761</td>\n",
              "      <td>-0.212447</td>\n",
              "      <td>-0.438114</td>\n",
              "      <td>0.185388</td>\n",
              "      <td>-0.316497</td>\n",
              "      <td>-0.038279</td>\n",
              "      <td>-0.226286</td>\n",
              "      <td>-0.171124</td>\n",
              "      <td>0.169253</td>\n",
              "      <td>-0.122906</td>\n",
              "      <td>-0.227463</td>\n",
              "      <td>0.125426</td>\n",
              "      <td>-0.145592</td>\n",
              "      <td>-0.092487</td>\n",
              "      <td>0.020526</td>\n",
              "      <td>0.005939</td>\n",
              "      <td>0.453325</td>\n",
              "      <td>-0.029890</td>\n",
              "      <td>-0.493776</td>\n",
              "      <td>0.158884</td>\n",
              "      <td>0.404596</td>\n",
              "      <td>0.169981</td>\n",
              "      <td>0.008827</td>\n",
              "      <td>0.235254</td>\n",
              "      <td>-0.129269</td>\n",
              "      <td>-0.295105</td>\n",
              "      <td>-0.020475</td>\n",
              "      <td>-0.165873</td>\n",
              "      <td>0.219995</td>\n",
              "      <td>0.231785</td>\n",
              "      <td>0.158701</td>\n",
              "      <td>-0.056250</td>\n",
              "      <td>0.168198</td>\n",
              "      <td>-0.303525</td>\n",
              "      <td>-0.013686</td>\n",
              "      <td>-0.199653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.208732</td>\n",
              "      <td>0.076510</td>\n",
              "      <td>0.320086</td>\n",
              "      <td>0.170445</td>\n",
              "      <td>0.125228</td>\n",
              "      <td>-0.278019</td>\n",
              "      <td>0.207956</td>\n",
              "      <td>0.401073</td>\n",
              "      <td>-0.182761</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.149643</td>\n",
              "      <td>-0.080398</td>\n",
              "      <td>-0.236592</td>\n",
              "      <td>0.491937</td>\n",
              "      <td>-0.174556</td>\n",
              "      <td>0.110451</td>\n",
              "      <td>-0.091751</td>\n",
              "      <td>0.246112</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.216716</td>\n",
              "      <td>0.076013</td>\n",
              "      <td>-0.160012</td>\n",
              "      <td>0.058508</td>\n",
              "      <td>0.218086</td>\n",
              "      <td>0.321029</td>\n",
              "      <td>-0.102378</td>\n",
              "      <td>-0.232576</td>\n",
              "      <td>0.113893</td>\n",
              "      <td>-0.173693</td>\n",
              "      <td>-0.112878</td>\n",
              "      <td>0.091271</td>\n",
              "      <td>0.192828</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.107810</td>\n",
              "      <td>-0.294061</td>\n",
              "      <td>-0.018130</td>\n",
              "      <td>0.225852</td>\n",
              "      <td>-0.399272</td>\n",
              "      <td>-0.033576</td>\n",
              "      <td>-0.159763</td>\n",
              "      <td>0.050007</td>\n",
              "      <td>0.002339</td>\n",
              "      <td>0.222264</td>\n",
              "      <td>-0.089890</td>\n",
              "      <td>0.166502</td>\n",
              "      <td>0.045417</td>\n",
              "      <td>-0.019429</td>\n",
              "      <td>-0.104035</td>\n",
              "      <td>0.175564</td>\n",
              "      <td>0.188577</td>\n",
              "      <td>-0.344213</td>\n",
              "      <td>0.021531</td>\n",
              "      <td>-0.358125</td>\n",
              "      <td>0.275160</td>\n",
              "      <td>-0.150722</td>\n",
              "      <td>-0.206600</td>\n",
              "      <td>0.099167</td>\n",
              "      <td>0.120465</td>\n",
              "      <td>-0.245994</td>\n",
              "      <td>-0.005484</td>\n",
              "      <td>-0.073102</td>\n",
              "      <td>-0.172824</td>\n",
              "      <td>-0.333555</td>\n",
              "      <td>-0.053223</td>\n",
              "      <td>-0.027466</td>\n",
              "      <td>-0.107598</td>\n",
              "      <td>-0.077475</td>\n",
              "      <td>-0.139521</td>\n",
              "      <td>0.049713</td>\n",
              "      <td>-0.136232</td>\n",
              "      <td>0.077064</td>\n",
              "      <td>0.075558</td>\n",
              "      <td>0.115107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.123669</td>\n",
              "      <td>-0.257363</td>\n",
              "      <td>0.033196</td>\n",
              "      <td>0.008619</td>\n",
              "      <td>-0.017399</td>\n",
              "      <td>-0.103638</td>\n",
              "      <td>0.456081</td>\n",
              "      <td>0.326836</td>\n",
              "      <td>-0.230241</td>\n",
              "      <td>-0.466926</td>\n",
              "      <td>-0.080469</td>\n",
              "      <td>-0.032098</td>\n",
              "      <td>-0.495967</td>\n",
              "      <td>0.242116</td>\n",
              "      <td>-0.190685</td>\n",
              "      <td>-0.164652</td>\n",
              "      <td>-0.030599</td>\n",
              "      <td>0.202306</td>\n",
              "      <td>0.141944</td>\n",
              "      <td>0.355540</td>\n",
              "      <td>0.097313</td>\n",
              "      <td>-0.227458</td>\n",
              "      <td>0.048716</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-0.201166</td>\n",
              "      <td>0.260034</td>\n",
              "      <td>-0.096229</td>\n",
              "      <td>0.021641</td>\n",
              "      <td>-0.462344</td>\n",
              "      <td>0.425936</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.264927</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.273580</td>\n",
              "      <td>-0.482840</td>\n",
              "      <td>0.258288</td>\n",
              "      <td>0.282566</td>\n",
              "      <td>-0.515495</td>\n",
              "      <td>-0.202641</td>\n",
              "      <td>-0.263403</td>\n",
              "      <td>0.029889</td>\n",
              "      <td>0.088701</td>\n",
              "      <td>0.473376</td>\n",
              "      <td>-0.011192</td>\n",
              "      <td>0.120483</td>\n",
              "      <td>-0.443588</td>\n",
              "      <td>-0.052380</td>\n",
              "      <td>-0.415964</td>\n",
              "      <td>-0.261790</td>\n",
              "      <td>-0.139862</td>\n",
              "      <td>0.245748</td>\n",
              "      <td>0.261483</td>\n",
              "      <td>-0.030431</td>\n",
              "      <td>0.052081</td>\n",
              "      <td>-0.306504</td>\n",
              "      <td>-0.273728</td>\n",
              "      <td>-0.020792</td>\n",
              "      <td>0.097888</td>\n",
              "      <td>0.174486</td>\n",
              "      <td>-0.323708</td>\n",
              "      <td>0.046917</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>-0.057291</td>\n",
              "      <td>0.344007</td>\n",
              "      <td>0.094371</td>\n",
              "      <td>-0.231564</td>\n",
              "      <td>-0.021173</td>\n",
              "      <td>0.259172</td>\n",
              "      <td>-0.309689</td>\n",
              "      <td>0.283516</td>\n",
              "      <td>0.162922</td>\n",
              "      <td>0.034134</td>\n",
              "      <td>-0.109642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.244448</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.056073</td>\n",
              "      <td>0.177915</td>\n",
              "      <td>-0.116873</td>\n",
              "      <td>0.027654</td>\n",
              "      <td>0.035792</td>\n",
              "      <td>0.454270</td>\n",
              "      <td>-0.086762</td>\n",
              "      <td>0.078437</td>\n",
              "      <td>-0.186529</td>\n",
              "      <td>-0.461408</td>\n",
              "      <td>-0.255728</td>\n",
              "      <td>0.120699</td>\n",
              "      <td>-0.171494</td>\n",
              "      <td>0.199815</td>\n",
              "      <td>-0.145918</td>\n",
              "      <td>0.098352</td>\n",
              "      <td>-0.009424</td>\n",
              "      <td>-0.153551</td>\n",
              "      <td>0.241185</td>\n",
              "      <td>-0.291973</td>\n",
              "      <td>0.094620</td>\n",
              "      <td>0.202216</td>\n",
              "      <td>0.073311</td>\n",
              "      <td>-0.192961</td>\n",
              "      <td>-0.155130</td>\n",
              "      <td>0.180534</td>\n",
              "      <td>-0.288508</td>\n",
              "      <td>-0.206734</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>0.045502</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.398336</td>\n",
              "      <td>-0.143743</td>\n",
              "      <td>0.106398</td>\n",
              "      <td>0.293327</td>\n",
              "      <td>-0.298150</td>\n",
              "      <td>-0.030243</td>\n",
              "      <td>-0.068640</td>\n",
              "      <td>-0.139920</td>\n",
              "      <td>0.025015</td>\n",
              "      <td>0.497330</td>\n",
              "      <td>-0.341690</td>\n",
              "      <td>0.156943</td>\n",
              "      <td>-0.084615</td>\n",
              "      <td>0.226769</td>\n",
              "      <td>0.209428</td>\n",
              "      <td>-0.470335</td>\n",
              "      <td>0.177939</td>\n",
              "      <td>0.054949</td>\n",
              "      <td>-0.359366</td>\n",
              "      <td>-0.063119</td>\n",
              "      <td>-0.164627</td>\n",
              "      <td>-0.354907</td>\n",
              "      <td>-0.324136</td>\n",
              "      <td>-0.053469</td>\n",
              "      <td>-0.013127</td>\n",
              "      <td>-0.406428</td>\n",
              "      <td>-0.287756</td>\n",
              "      <td>0.113375</td>\n",
              "      <td>-0.484681</td>\n",
              "      <td>-0.263360</td>\n",
              "      <td>0.346316</td>\n",
              "      <td>0.216065</td>\n",
              "      <td>0.105335</td>\n",
              "      <td>0.148456</td>\n",
              "      <td>-0.006926</td>\n",
              "      <td>0.009776</td>\n",
              "      <td>-0.055775</td>\n",
              "      <td>-0.018340</td>\n",
              "      <td>0.206965</td>\n",
              "      <td>-0.192643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 776 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  is_hot_word  is_hashtag  ...       765       766       767\n",
              "0   0            1           0  ... -0.197440  0.058177 -0.145312\n",
              "1   2            1           1  ... -0.303525 -0.013686 -0.199653\n",
              "2   3            1           0  ...  0.077064  0.075558  0.115107\n",
              "3   9            0           1  ...  0.162922  0.034134 -0.109642\n",
              "4  11            0           0  ... -0.018340  0.206965 -0.192643\n",
              "\n",
              "[5 rows x 776 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 334
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qi74p2CZHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_to_submit = pd.DataFrame(data_for_testing.iloc[:,0])\n",
        "test_to_submit.columns = ['id']\n",
        "test_to_submit.head()\n",
        "test_df_model_fortraining_noid = data_for_testing.iloc[:,1:]\n",
        "##########\n",
        "target_predicted = mlp_predict(test_df_model_fortraining_noid, mlp)\n",
        "target_predicted.shape\n",
        "target = pd.DataFrame((target_predicted>0.5).reshape(-1))\n",
        "target.columns = ['target']\n",
        "target['target'] = target['target'].apply(lambda x: int(x))\n",
        "submission = pd.DataFrame([test_to_submit['id'],target['target']]).T\n",
        "submission.head()\n",
        "submission.to_csv('submission.csv', index = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ve1rsvVFbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "1f09f966-ddb5-4345-b145-b15e449ae745"
      },
      "source": [
        "ls"
      ],
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mbert_output\u001b[0m/                                     'NLP Baseline.ipynb'\n",
            " mlp-weights-best.hdf5                             res_majority.csv\n",
            "'New changes to improve baseline accuracy.ipynb'   submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1TlUEkefTfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}