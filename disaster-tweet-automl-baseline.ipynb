{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/automlwrapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/automlwrapper')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\nfrom automlwrapper import AutoMLWrapper","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nPROJECT_ID = 'kaggle-disaster-tweets'\nbucket_name = f'{PROJECT_ID}-lcm'\n\nregion = 'us-central1' # Region must be us-central1\ndataset_display_name = 'kaggle_tweets'\nmodel_display_name = 'kaggle_tweets_model1'\n\nstorage_client = storage.Client(project=PROJECT_ID)\nclient = automl.AutoMlClient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nnlp_test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndef callback(operation_future):\n    result = operation_future.result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data spelunking"},{"metadata":{},"cell_type":"markdown","source":"#### How often does 'fire' come up in this dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Does the presence of the word 'fire' help determine whether the tweets here are real or false?"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)].target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df.loc[(nlp_train_df['text'].str.contains('fire', na=False, case=False)) & (nlp_train_df['target'] == 0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GCS upload/download utilities"},{"metadata":{},"cell_type":"markdown","source":"#### These functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML"},{"metadata":{"trusted":true},"cell_type":"code","source":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs://' + bucket_name + '/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name, destination_directory, file_name, prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bucket = storage.Bucket(storage_client, name=bucket_name)\nif not bucket.exists():\n    bucket.create(location=region)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export to CSV and upload to GCS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the text body and the target value, for sending to AutoML NL\nnlp_train_df[['text','target']].to_csv('train.csv', index=False, header=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df[['id','text','target']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_gcs_path = 'uploads/kaggle_getstarted/full_train.csv'\nupload_blob(bucket_name, 'train.csv', training_gcs_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create our class instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region=region, \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create (or retreive) dataset"},{"metadata":{},"cell_type":"markdown","source":"### Check to see if this dataset already exists. If not, create it"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found')\n    amw.create_dataset()\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kick off the training for the model"},{"metadata":{},"cell_type":"markdown","source":"### And retrieve the training info after completion. Start model deployment."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not amw.get_model_by_display_name():\n    amw.train_model()\namw.deploy_model()\namw.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amw.model_full_path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{},"cell_type":"markdown","source":"#### Note that prediction will not run until deployment finishes, which takes a bit of time. However, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\npredictions_df = amw.get_predictions(nlp_test_df, \n                                     input_col_name='text', \n#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (optional) Undeploy model"},{"metadata":{},"cell_type":"markdown","source":"### Undeploy the model to stop charges"},{"metadata":{"trusted":true},"cell_type":"code","source":"amw.undeploy_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions to the competition!"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls -l submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}