{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T6QMeQ4Rml_I",
    "outputId": "3079c8fb-934d-4bd8-b26d-c8a98d19356e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8MB 41kB/s \n",
      "\u001b[?25hCollecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 39.4MB/s \n",
      "\u001b[?25hCollecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.5)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 46.1MB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 57.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (42.0.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/5f/a1a02695b96d0e09c38abf7d1576b137979cea3d060d60891622cf61276d/google_auth-1.10.1-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 13.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow-gpu, keras, tqdm\n",
      "  Found existing installation: google-auth 1.4.2\n",
      "    Uninstalling google-auth-1.4.2:\n",
      "      Successfully uninstalled google-auth-1.4.2\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: Keras 2.2.5\n",
      "    Uninstalling Keras-2.2.5:\n",
      "      Successfully uninstalled Keras-2.2.5\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "Successfully installed google-auth-1.10.1 keras-2.3.1 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 tqdm-4.41.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google",
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('2.1.0', '/device:GPU:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-gpu keras tqdm\n",
    "import tensorflow as tf\n",
    "tf.__version__, tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "X4MDltsBmxTN",
    "outputId": "283fe8a6-1a30-4f17-d73a-c978d5bbc941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "7PaHcghynXRt",
    "outputId": "9e48d0de-8738-4243-8094-4329b0c68c3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/env/python',\n",
       " '/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/root/.ipython',\n",
       " '/content/drive/My Drive/External-Tech/Codes/bert']"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/External-Tech/Codes/bert')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "codVkRstoVpx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfAdT6NxprFM"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Kaggle/nlp-getting-started/input'\n",
    "\n",
    "BERT_PATH = '/content/drive/My Drive/External-Tech/Models/bert-base-uncased-tf'\n",
    "TOKENIZER = FullTokenizer(os.path.join(BERT_PATH, 'assets', 'vocab.txt'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "qSNy7Qpoq3G6",
    "outputId": "567d3e8f-7142-45f0-ca6a-c8ee996aa825"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>non_stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>mean_sentence_length</th>\n",
       "      <th>number_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>typo_count</th>\n",
       "      <th>slang_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>All residents asked to ishelter in place' are ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       keyword  ... typo_count slang_count\n",
       "0   1  no%20keyword  ...          0           0\n",
       "1   4  no%20keyword  ...          2           0\n",
       "2   5  no%20keyword  ...          3           0\n",
       "3   6  no%20keyword  ...          2           0\n",
       "4   7  no%20keyword  ...          1           0\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'train_extra_feat.csv'))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "hhKBX3ARrTxi",
    "outputId": "891d960e-b630-458e-ffa9-e2b69ffc384c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>non_stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>mean_sentence_length</th>\n",
       "      <th>number_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>typo_count</th>\n",
       "      <th>slang_count</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.222222</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heard about earthquake is different cities, st...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.105263</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Apocalypse lighting. Spokane wildfires</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>no%20keyword</td>\n",
       "      <td>no%20location</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       keyword       location  ... typo_count  slang_count  emoji_count\n",
       "0   0  no%20keyword  no%20location  ...          0            0            0\n",
       "1   2  no%20keyword  no%20location  ...          2            0            0\n",
       "2   3  no%20keyword  no%20location  ...          3            0            0\n",
       "3   9  no%20keyword  no%20location  ...          3            0            0\n",
       "4  11  no%20keyword  no%20location  ...          1            0            0\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(os.path.join(DATA_PATH, 'test_extra_feat.csv'))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0AOFzKb9r0U3",
    "outputId": "cf37a27b-d969-4645-91b5-841b996616ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 21), (3263, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LIDGEepfr5MU",
    "outputId": "f7b49e5b-857c-4587-d274-b0ddf6355aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BNzsXIXWuBwt",
    "outputId": "7d9265e5-5e31-43ba-f347-cf54cedb95e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yW7mLkNuJXv"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IseJ_gKr6o5"
   },
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    return TOKENIZER.tokenize(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjP45rUitqxz"
   },
   "outputs": [],
   "source": [
    "def get_ids(tokenized_text):\n",
    "    token_ids = TOKENIZER.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = token_ids + [0] * (MAX_SEQUENCE_LENGTH - len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0pQC9_Hyisq"
   },
   "outputs": [],
   "source": [
    "def get_masks(tokenized_text):\n",
    "    return [1] * len(tokenized_text) + [0] * (MAX_SEQUENCE_LENGTH - len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nkoCTLAzq6V"
   },
   "outputs": [],
   "source": [
    "def get_segments(tokenized_text):\n",
    "    return [0] * MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWm4POaitWzG"
   },
   "outputs": [],
   "source": [
    "def convert_to_bert_inputs(tokenized_text):\n",
    "    if len(tokenized_text) > MAX_SEQUENCE_LENGTH - 2:\n",
    "        print('Removing extra in ...')\n",
    "        print(len(tokenized_text), tokenized_text)\n",
    "        print()\n",
    "        tokenized_text = tokenized_text[:MAX_SEQUENCE_LENGTH - 2]\n",
    "    tokens = ['[CLS]'] + tokenized_text + ['[SEP]']\n",
    "    input_ids = get_ids(tokens)\n",
    "    input_masks = get_masks(tokens)\n",
    "    input_segments = get_segments(tokens)\n",
    "    return input_ids, input_masks, input_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1scuHvPjsgDf"
   },
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, col):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    with tqdm(total=len(df[col].values)) as pbar:\n",
    "        for row in df[col].values:\n",
    "            tokenized_text = TOKENIZER.tokenize(row)\n",
    "            ids, masks, segments = convert_to_bert_inputs(tokenized_text)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "dTd5rmq7skvI",
    "outputId": "08422dff-0bfe-40e7-bd68-295d0cb0249c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1249/7613 [00:00<00:01, 4035.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra in ...\n",
      "54 ['ca', '##go', '##v', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?', 'vp']\n",
      "\n",
      "Removing extra in ...\n",
      "49 ['ap', '##hia', '##bet', '##a1', '##90', '##7', 'w', 'u', '##gli', '##ness', 'due', '2', 'your', \"'\", 'ugly', \"'\", 'ames', '##oc', '##ial', '##act', '##ion', 'fra', '##t', 'is', 'bio', '##ter', '##ror', '##ism', '##i', 'am', 'she', 'who', 'is', 'fbi', 'id', 'you', '$', 'to', '##le', '##wan', '##t', \"'\", 'g', 'another', 'in', 'my', 'home', '.', 'abc']\n",
      "\n",
      "Removing extra in ...\n",
      "52 ['howard', '##u', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?']\n",
      "\n",
      "Removing extra in ...\n",
      "54 ['cs', '##pan', '##w', '##j', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?']\n",
      "\n",
      "Removing extra in ...\n",
      "50 ['rink', '##yd', '##nk', '##2', 'za', '##iba', '##tsu', '##ne', '##ws', 'neo', '##pro', '##gre', '##ssi', '##ve', '##1', 'when', 'push', '##2', '##le', '##ft', 'talk', \"'\", 'ecology', \"'\", 'and', 'amp', \"'\", 'human', 'rt', '##s', \"'\", 'and', 'amp', 'would', '##em', '##oc', '##rac', '##y', \"'\", '.', 'war', 'af', '##gh', '##et', '##c', \"'\", 'left', \"'\", 'humanitarian', 'bombing']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2476/7613 [00:00<00:01, 4064.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra in ...\n",
      "62 ['info', 's', '.', 'w', '##nd', '03', '##0', '6', '.', 'cl', '##d', 'sc', '##t', '##01', '##4', 'bk', '##n', '##0', '##32', '.', 'ex', '##p', 'ins', '##t', 'ap', '##ch', '.', 'r', '##wy', '05', '.', 'cu', '##rf', '##ew', 'in', 'op', '##er', 'until', '203', '##0', 'z', '.', 'taxi', '##ways', 'fox', '##tro', '##t', '5', 'and', 'amp', 'fox', '##tro', '##t', '6', 'na', '##vb', '##l', '.', 't', '##mp', '10', '.']\n",
      "\n",
      "Removing extra in ...\n",
      "58 ['info', 'r', '.', 'cu', '##rf', '##ew', 'in', 'op', '##er', 'until', '203', '##0', 'z', '.', 'taxi', '##ways', 'fox', '##tro', '##t', '5', 'and', 'amp', 'fox', '##tro', '##t', '6', 'na', '##vb', '##l', '.', 'w', '##nd', '06', '##0', '5', '.', 'ex', '##p', 'ins', '##t', 'ap', '##ch', '.', 'r', '##wy', '05', '.', 'damp', '.', 't', '##mp', '10', '.', 'q', '##nh', '102', '##8', '.']\n",
      "\n",
      "Removing extra in ...\n",
      "62 ['info', 'u', '.', 'cl', '##d', 'sc', '##t', '##01', '##2', 'bk', '##n', '##0', '##25', '.', 'ex', '##p', 'ins', '##t', 'ap', '##ch', '.', 'r', '##wy', '05', '.', 'cu', '##rf', '##ew', 'in', 'op', '##er', 'until', '203', '##0', 'z', '.', 'taxi', '##ways', 'fox', '##tro', '##t', '5', 'and', 'amp', 'fox', '##tro', '##t', '6', 'na', '##vb', '##l', '.', 't', '##mp', '10', '.', 'w', '##nd', '03', '##0', '6', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:01<00:00, 4161.33it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs_train = compute_input_arrays(df_train, 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "CSf5NkoP1uzF",
    "outputId": "21d0f810-7c95-450d-b729-75f3f6ab066e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 802/3263 [00:00<00:00, 3954.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing extra in ...\n",
      "53 ['usc', '##our', '##t', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?']\n",
      "\n",
      "Removing extra in ...\n",
      "52 ['harvard', '##u', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?']\n",
      "\n",
      "Removing extra in ...\n",
      "53 ['ga', '##court', '##s', 'if', '90', '##bl', '##ks', 'and', 'amp', '8', '##w', '##ht', '##s', 'col', '##lu', '##ded', '2', 'take', 'w', '##ht', 'f', 'usa', '##go', '##v', 'au', '##th', 'hostage', 'and', 'amp', '2', 'make', 'her', 'look', 'b', '##lk', 'w', 'bio', '##ter', '##ror', '##ism', 'and', 'amp', 'use', 'her', 'l', '##gl', 'org', 'id', '##is', 'id', 'still', 'hers', '?']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [00:00<00:00, 3904.43it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs_test = compute_input_arrays(df_test, 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h7cKnOrzZ6ax",
    "outputId": "bee7ad52-e0cf-4f00-a60e-181049169916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 50)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KLAnoxaW6KN"
   },
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32,\n",
    "                                       name='input_word_ids')\n",
    "input_masks = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, \n",
    "                                    name='input_masks')\n",
    "input_segments = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32,\n",
    "                                       name='input_segments')\n",
    "\n",
    "bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n",
    "_, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n",
    "output = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_word_ids, input_masks, input_segments], \n",
    "    outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEGtbkAjYx8H"
   },
   "outputs": [],
   "source": [
    "outputs_train = model.predict(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6TBjIxdaqoN"
   },
   "outputs": [],
   "source": [
    "outputs_test = model.predict(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Duu__zca0ze"
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_PATH, 'bert-embeddings-train.npy'), outputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFclZ-5CbmLE"
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_PATH, 'bert-embeddings-test.npy'), outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zSV-_xfcoDt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
